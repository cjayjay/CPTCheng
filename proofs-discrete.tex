\todoj[inline]{Fix notation, fix claims to include both positive and negative parts in CPT}
Before exploring on the convergence speed to the sample estimator, we introduce Hoeffding inequality which will be useful later on. 

Introducing the notations:
Let $F_k = \sum_{i=k}^K p_i$, and naturally $\hat{F_k} =\sum_{i=k}^K \hat{p_i}= \frac{1}{n} \sum_{i=1}^n I_{\{U\geq  x_k\} } $. 


\begin{lemma}
Let $Y_1,...Y_n$ be \emph{independent random variables} satisfying $P(a\leq Y_i \leq b)= 1,$ each i, where $a<b.
$Then for $t>0$,
$$P(\left|\sum_{i=1}^n Y_i -\sum_{i=1}^n E(Y_i)\right| \geq nt ) \leq 2\exp{\{-2nt^2 /(b-a)^2\}} $$
\end{lemma}

The Hoeffding inequality suggests the following proposition:
\begin{proposition}
Let $\hat{F_k} = \frac{1}{n} \sum_{i=1}^n I_{\{U_i \geq x_k\}}$, Then for every $\epsilon >0$, 
$$P(|\hat{F_k}-F_k| > \epsilon) \leq 2 e^{-2n \epsilon^2} $$
\end{proposition}
\begin{proof}
Notice the random variables $I_{(U_i \geq  x_k) }$ are independent to each other for each i, and it is bounded by 1. 
The probability $P(\left|P_k- P_k \right| > \epsilon)$ is equal to 
\begin{align*}
&
P(\left|\hat{F_k}- F_k \right| > \epsilon) \\ & = P(\left| \frac{1}{n} \sum_{i=1}^n I_{\{U_i \geq
x_k\}} - \frac{1}{n} \sum_{i=1}^n E(I_{\{U_i \geq x_k\}}) \right| > \epsilon) \\ & = P(\left|
\sum_{i=1}^n I_{\{U_i \geq x_k\}} - \sum_{i=1}^n E(I_{\{U_i \geq x_k\}}) \right| > n\epsilon) \\ &
    \leq 2e^{-2n \epsilon^2}
\end{align*}

\end{proof}
Proposition 1.2 gives a convergence rate of $\hat{p_k}$ to the value $p_k$, regardless of what k is. 
Additionally, since w is locally Lipschitz as indicated in the paper, we can explore the sample complexity of the estimation algorithm in a fairly easy manner. 
Before exhibiting the proposition for sample complexity, couple of notations would be introduced: 
\\
We denote $L=\max\{L_k, k=2...K\} $, since one don't have to care the case when of $w(1)=1$. The $L_k$ is the local Lipschitz constant of function $w(x)$ at point 
$P_k$, 
$A=\max\{x_k, k=1...K\}$, $\delta =\min\{\delta_k\}$, where $\delta_k$ is the half length of the interval that locally Lipschitz of the point $F_k$ holds.
The following proposition is valid: 
\begin{proposition}
Suppose $\hat{F_k}$ is the empirical estimation of $F_k$, then 
$$P(\left| \sum_{i=1}^K x_k w(\hat{F_k}) - \sum_{i=1}^K x_k w(F_k) \right| >\epsilon) < K\cdot (
e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2}) $$ 
\end{proposition}

\begin{proof}
Observe that

\begin{align*}
&
P(\left| \sum_{k=1}^K x_k w(\hat{F_k}) - \sum_{k=1}^K x_k w(F_k) \right| >\epsilon) \\ & = P (
\bigcup_{k=1}^K \left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & \leq
    \sum_{k=1}^K P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K})
\end{align*}
Notice that $\forall k =1,....K$
$[{p_k}- \delta, {p_k}+\delta)$,
the function $w$ is locally Lipschitz with common constant $L$.
Therefore, for each k, we can decompose the probability as 
\begin{align*}
& P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & = P ( [ \left| F_k -
\hat{F_k} \right| >\delta ] \bigcap [ \left| x_k w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac
{\epsilon} {K}) + P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k
    w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac {\epsilon} {K}) \\ & \leq P ( \left| F_k - \hat{F_k}
    \right| >\delta) + P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k
    w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac {\epsilon} {K})
\end{align*}
 
According to the property of locally Lipschitz continuous,
we have
\begin{align*}
& P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k w(\hat{F_k}) -x_k w(F_k)
\right| ] > \frac {\epsilon} {K}) \\ & \leq P(x_k L \left| F_k - \hat{F_k} \right| > \frac
    {\epsilon} {K}) \leq e^ {-\epsilon\cdot 2n /(K L x_k)^2} \leq e^ {-\epsilon\cdot 2n /(K L A)^2}
    \text{     for    } \forall k
\end{align*}
And similarly,
\begin{align*}
& P(\left| F_k - \hat{F_k} \right| > \delta) \\ & \leq e^{-\delta^2 /2n} \text{    for     } \forall
    n
\end{align*}
And as a result,
\begin{align*}
& P(\left| \sum_{k=1}^K x_k w(\hat{F_k}) - \sum_{k=1}^K x_k w(F_k) \right| >\epsilon) \\ & \leq
\sum_{k=1}^K P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & \leq
             \sum_{k=1}^K e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 \cdot 2n/ (KLA)^2} \\ & =K\cdot
    (e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 \cdot 2n/ (KLA)^2})
\end{align*}

\end{proof}

As a result, we will derive the following lemma associated to sample complexity:
\begin{lemma}
Denote $M=\min(\delta^2, \epsilon^2/(KLA)^2)$, for any $\epsilon$, $\rho$, we have
\begin{align}
P(\left|\sum_{i=1}^K x_k \cdot(w(\hat{F_k})- w(\hat{F_{k+1}}) ) -  \sum_{i=1}^K x_k \cdot(w(F_k)-
w(F_{k+1}) ) \right| \leq \epsilon) > 1-\rho \text{      } \forall n> \frac{\ln(\frac{4K}{a})} { M} 
\end{align}
\begin{proof}
By repeating the identical procedure one can show that
$$P(\left| \sum_{i=1}^K x_k w(\hat{F_{k+1} }) - \sum_{i=1}^K x_k w(F_{k+1}) \right| >\epsilon) <
K\cdot ( e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2})
$$
Therefore,
\begin{align*}
& P(\left|\sum_{i=1}^K x_k \cdot(w(\hat{F_k})- w(\hat{F_{k+1}}) ) -  \sum_{i=1}^K x_k \cdot(w(F_k)-
w(F_{k+1}) ) \right| > \epsilon) \\ & \leq P(\left|\sum_{i=1}^K x_k \cdot(w(\hat{F_k})) -
    \sum_{i=1}^K x_k \cdot(w(F_k)) \right| > \epsilon/2) + P(\left|\sum_{i=1}^K x_k
    \cdot(w(\hat{F_{k+1})}) -  \sum_{i=1}^K x_k \cdot(w(F_{k+1})) \right| > \epsilon/2) \\ & \leq 2K
    (e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2})
\end{align*}
And by introducing the notation $M=\min(\delta^2, \epsilon^2/(KLA)^2)$, one can conclude the sample complexity stated in lemma.
\end{proof}
\end{lemma}