<<<<<<< HEAD
\paragraph{Background.}
Here we assume that the r.v. $X$ is discrete valued.
Let $p_i, i=1,\ldots,K$ denote the probability of incurring a gain/loss $x_i, i=1,\ldots,K$. %Assume $x_1 \le \ldots \le x_K$. 
Given a utility function $u$ and weighting function $w$, \textit{\textbf{Prospect theory}} (PT) value is defined as $V(X) = \sum_{i=1}^K u(x_i) w(p_i)$. 
As explained in the introduction, the idea is to take an utility function that is $S$-shaped, so that it satisfies the \textit{diminishing sensitivity}  property. 
If we take the weighting function $w$ to be the identity, then one recovers the classic expected utility. A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see \cite{kahneman1979prospect}, \cite{fennema1997original} for a justification, in particular via empirical tests using human subjects).
However, PT is lacking in some theoretical aspects as it violates first-order \textit{stochastic dominance}.\footnote{Consider the following example from \cite{fennema1997original}: Suppose there are $20$ prospects (outcomes) ranging from $-10$ to $180$, each with probability $0.05$. If the weight function is such that $w(0.05) > 0.05$, then it uniformly overweights all \textit{low-probability} prospects and the resulting PT value is higher than the expected value $85$. This violates stochastic dominance, since a shift in the probability mass from bad outcomes did not result in a better prospect.}

CPT uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as 
$x_1\le \ldots \le x_l \le 0 \le x_{l+1} \le \ldots \le x_K$. Then, the CPT-value is defined as 
\begin{align}
\label{eq:cpt-discrete}
V(X) = &(u^-(x_1))\cdot w^-(p_1) 
+\sum_{i=2}^l u^-(x_i) \Big(w^-(\sum_{j=1}^i p_j) - w^-(\sum_{j=1}^{i-1} p_j)\Big) 
\\&
 + \sum_{i=l+1}^{K-1} u^+(x_i) \Big(w^+(\sum_{j=i}^K p_j) - w^-(\sum_{j=i+1}^K p_j) \Big)
 + u^+(x_K)\cdot w^+(p_K), 
\end{align} 
where $u^+, u^-$ are utility functions and $w^+, w^-$ are weight functions corresponding to gains and losses, respectively. The utility functions $u^+$ and $u^-$ are non-decreasing, while the weight functions are continuous, non-decreasing and have the range $[0,1]$ with $w^+(0)=w^-(0)=0$ and $w^+(1)=w^-(1)=1$ . 
Unlike PT, the CPT-value does not violate stochastic dominance.\footnote{In the aforementioned example, increasing $w^-(0.05)$ and $w^+(0.05)$ does not impact outcomes other than those on the extreme, i.e., $-10$ and $180$, respectively. For instance, the weight for outcome $100$ would be $w^+(0.45) - w^+(0.40)$. Thus, CPT formalizes the intuitive notion that humans are sensitive to extreme outcomes and relatively insensitive to intermediate ones.}

\paragraph{Estimation scheme.} 
Let $\hat{p_k}= \frac{1}{n} \sum_{i=1}^n I_{\{U =x_k\}}$. Then, we estimate $V(X)$ as follows:
\begin{align}
 \label{eq:cpt-discrete-est}
\hat V_n(X) = 
u^-(x_1)\cdot w^-(\hat p_1)+
& \sum_{i=2}^l u^-(x_i) \Big(w^-(\sum_{j=1}^i \hat p_j) - w^-(\sum_{j=1}^{i-1} \hat p_j)\Big) 
\\
&
+ \sum_{i=l+1}^{K-1} u^+(x_i) \Big(w^+(\sum_{j=i}^K \hat p_j) - w^-(\sum_{j=i+1}^K \hat p_j) \Big)+ u^+(x_K)\cdot w^+(\hat p_K).
\end{align}
Owing to the fact that $\hat{p_k}$ converge a.e to $p_k=P(X_i=x_k)$, with $X_i$ be the sample of $X$ the above estimator obtains strong consistency property according to continuous mapping theorem. 

\paragraph{Sample Complexity}
Before exploring on the convergence speed to the sample estimator, it is necessary to introduce Hoeffding's inequality:
\begin{lemma}
Let $Y_1,...Y_n$ be \emph{independent random variables} satisfying $P(a\leq Y_i \leq b)= 1,$ each i, where $a<b.
$Then for $t>0$,
$$P(\left|\sum_{i=1}^n Y_i -\sum_{i=1}^n E(Y_i)\right| \geq nt ) \leq 2\exp{\{-2nt^2 /(b-a)^2\}} $$
\end{lemma}

\noindent \textbf{Notations:} We will introduce 
\[
F_k = 
\begin{cases}
   \sum_{i=1}^k p_k & \text{if   } k \leq l \\
   \sum_{i=k}^K p_k & \text{if  }  k > l
\end{cases}  
\]

and $\hat F_k$ retains the same form as $F_k$ by replacing $p_k$ by $\hat p_k$ 
\\

\noindent The Hoeffding inequality suggests the following proposition:
\begin{proposition}
Let $F_k$ and $\hat F_k$ as introduced above, Then for every $\epsilon >0$, 
$$P(|\hat{F_k}-F_k| > \epsilon) \leq 2 e^{-2n \epsilon^2} $$
\end{proposition}
\begin{proof}
We focus on the case when $k > l$,and the case of $k \leq l$ will be proved through the same fashion.
Notice that when $k>l$,  $\hat F_k =I_{(U_i \geq  x_k) }$ and the random variables are independent to each other for each i, and it is bounded by 1. 
The probability $P(\left|F_k- F_k \right| > \epsilon)$ is equal to 
\begin{align*}
&
P(\left|\hat{F_k}- F_k \right| > \epsilon) \\ & = P(\left| \frac{1}{n} \sum_{i=1}^n I_{\{U_i \geq
x_k\}} - \frac{1}{n} \sum_{i=1}^n E(I_{\{U_i \geq x_k\}}) \right| > \epsilon) \\ & = P(\left|
\sum_{i=1}^n I_{\{U_i \geq x_k\}} - \sum_{i=1}^n E(I_{\{U_i \geq x_k\}}) \right| > n\epsilon) \\ &
    \leq 2e^{-2n \epsilon^2}
\end{align*}

\end{proof}
Proposition 5 gives a convergence rate of $\hat{F_k}$ to the value $F_k$, regardless of what k is. 
Additionally, since $w^+$ and $w^-$ are both locally Lipschitz as indicated in the paper, we can explore the sample complexity of the estimation algorithm through the following lemma: 

\begin{theorem}[Sample Complexity: discrete case]
\label{thm:sample-complexity}
Denote $L=\max\{L_k, k=2...K\} $, where $L_k$ is the local Lipschitz constant of function $w^-(x)$ at points
$F_k$, where $k=1,...l$, and of function $w^+(x)$ at points $k=l+1,...K$. 
And let $A=\max\{x_k, k=1...K\}$, $\delta =\min\{\delta_k\}$, where $\delta_k$ is the half length of the interval centered at point $F_k$ where locally Lipschitz property with constant $L_k$ holds.
For any $\epsilon$, $\rho$,let $M=\min(\delta^2, \epsilon^2/(KLA)^2)$, and we have 
\begin{align}
P(\left|
\hat V_n(X) -V(X)
\right| \leq \epsilon) > 1-\rho \text{        ,} \forall n> \frac{\ln(\frac{4K}{a})} { M} 
\end{align}

\end{theorem}

Before proving the preceding theorem, we will introduce the following proposition : 
\begin{proposition}
Following the same notations and conditions introduced in theorem 2, and assume that
$w$ is Locally Lipschitz continuous with constants $L_1,....L_K$ on the points $F_1,....F_K$ as written in the statement of theorem 2, we have
$$P(\left| \sum_{i=1}^K x_k w(\hat{F_k}) - \sum_{i=1}^K x_k w(F_k) \right| >\epsilon) < K\cdot (
e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2}) $$ 
\end{proposition}

\begin{proof}
Observe that

\begin{align*}
&
P(\left| \sum_{k=1}^K x_k w(\hat{F_k}) - \sum_{k=1}^K x_k w(F_k) \right| >\epsilon) \\ & = P (
\bigcup_{k=1}^K \left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & \leq
    \sum_{k=1}^K P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K})
\end{align*}
Notice that $\forall k =1,....K$
$[{p_k}- \delta, {p_k}+\delta)$,
the function $w$ is locally Lipschitz with common constant $L$.
Therefore, for each k, we can decompose the probability as 
\begin{align*}
& P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & = P ( [ \left| F_k -
\hat{F_k} \right| >\delta ] \bigcap [ \left| x_k w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac
{\epsilon} {K}) + P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k
    w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac {\epsilon} {K}) \\ & \leq P ( \left| F_k - \hat{F_k}
    \right| >\delta) + P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k
    w(\hat{F_k}) -x_k w(F_k) \right| ] > \frac {\epsilon} {K})
\end{align*}
 
According to the property of locally Lipschitz continuous,
we have
\begin{align*}
& P ( [ \left| F_k - \hat{F_k} \right| \leq\delta ] \bigcap [ \left| x_k w(\hat{F_k}) -x_k w(F_k)
\right| ] > \frac {\epsilon} {K}) \\ & \leq P(x_k L \left| F_k - \hat{F_k} \right| > \frac
    {\epsilon} {K}) \leq e^ {-\epsilon\cdot 2n /(K L x_k)^2} \leq e^ {-\epsilon\cdot 2n /(K L A)^2}
    \text{     for    } \forall k
\end{align*}
And similarly,
\begin{align*}
& P(\left| F_k - \hat{F_k} \right| > \delta) \\ & \leq e^{-\delta^2 /2n} \text{    for     } \forall
    n
\end{align*}
And as a result,
\begin{align*}
& P(\left| \sum_{k=1}^K x_k w(\hat{F_k}) - \sum_{k=1}^K x_k w(F_k) \right| >\epsilon) \\ & \leq
\sum_{k=1}^K P (\left| x_k w(\hat{F_k}) -x_k w(F_k) \right| > \frac {\epsilon} {K}) \\ & \leq
             \sum_{k=1}^K e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 \cdot 2n/ (KLA)^2} \\ & =K\cdot
    (e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 \cdot 2n/ (KLA)^2})
\end{align*}

\end{proof}

By giving the above proposition, we can prove theorem 2:
\begin{proof}[Proof of theorem 2:]
Since the functions $w^-$ and $w^+$ all locally Lipschitz in the according points and with the according constants introduced in theorem 2, it is suggestive only to write $w$ uniformly in place of $w^-$ and $w^+$ in the separate cases $1\leq k\leq l$ and $k> l$, in order to avoid unnecessary technicalities. 
The proof is equivalently to show that
\begin{align}
P(\left|\sum_{i=1}^K u(x_k) \cdot(w(\hat{F_k})- w(\hat F_{k+1}) )
-  
\sum_{i=1}^K u(x_k) \cdot(w(F_k)- w(F_{k+1}) )
\right| \leq \epsilon) > 1-\rho
\text{      ,     } \forall n> \frac{\ln(\frac{4K}{a})} { M} 
\end{align}
under which $w$ is Locally Lipschitz continuous with constants $L_1,....L_K$ on the points $F_1,....F_K$ as written in the statement of theorem 2.
Observe that by repeating the identical procedure in the proof of proposition 6 one can show that
$$P(\left| \sum_{i=1}^K x_k w(\hat F_{k+1}) - \sum_{i=1}^K x_k w(F_{k+1}) \right| >\epsilon) <
K\cdot ( e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2})
$$
Therefore,
\begin{align*}
& P(\left|\sum_{i=1}^K x_k \cdot(w(\hat{F_k})- w(\hat F_{k+1}) ) -  \sum_{i=1}^K x_k \cdot(w(F_k)-
w(F_{k+1}) ) \right| > \epsilon) \\ & \leq P(\left|\sum_{i=1}^K x_k \cdot(w(\hat{F_k})) -
    \sum_{i=1}^K x_k \cdot(w(F_k)) \right| > \epsilon/2) + P(\left|\sum_{i=1}^K x_k
    \cdot(w(\hat F_{k+1})) -  \sum_{i=1}^K x_k \cdot(w(F_{k+1})) \right| > \epsilon/2) \\ & \leq 2K
    (e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2})
\end{align*}
And by introducing the notation $M=\min(\delta^2, \epsilon^2/(KLA)^2)$, one can conclude the sample complexity property stated in the theorem.




\end{proof}

=======
\paragraph{Background.}
Here we assume that the r.v. $X$ is discrete valued.
Let $p_i, i=1,\ldots,K$ denote the probability of incurring a gain/loss $x_i, i=1,\ldots,K$. %Assume $x_1 \le \ldots \le x_K$. 
Given a utility function $u$ and weighting function $w$, \textit{\textbf{Prospect theory}} (PT) value is defined as $V(X) = \sum_{i=1}^K u(x_i) w(p_i)$. 
As explained in the introduction, the idea is to take an utility function that is $S$-shaped, so that it satisfies the \textit{diminishing sensitivity}  property. 
If we take the weighting function $w$ to be the identity, then one recovers the classic expected utility. A general weight function inflates low probabilities and deflates high probabilities and this has been shown to be close to the way humans make decisions (see \cite{kahneman1979prospect}, \cite{fennema1997original} for a justification, in particular via empirical tests using human subjects).
However, PT is lacking in some theoretical aspects as it violates first-order \textit{stochastic dominance}.\footnote{Consider the following example from \cite{fennema1997original}: Suppose there are $20$ prospects (outcomes) ranging from $-10$ to $180$, each with probability $0.05$. If the weight function is such that $w(0.05) > 0.05$, then it uniformly overweights all \textit{low-probability} prospects and the resulting PT value is higher than the expected value $85$. This violates stochastic dominance, since a shift in the probability mass from bad outcomes did not result in a better prospect.}

CPT uses a similar measure as PT, except that the weights are a function of cumulative probabilities. First, separate the gains and losses as 
$x_1\le \ldots \le x_l \le 0 \le x_{l+1} \le \ldots \le x_K$. Then, the CPT-value is defined as 
\begin{align}
\label{eq:cpt-discrete}
V(X) = & \sum_{i=l+1}^{K-1} u^+(x_i) \Big(w^+(\sum_{j=i}^K p_j) - w^+(\sum_{j=i+1}^K p_j) \Big)
- \sum_{i=1}^{l} u^-(x_i) \Big(w^-(\sum_{j=i}^l p_j) - w^-(\sum_{j=i+1}^l p_j) \Big), 
\end{align} 
where $u^+, u^-$ are utility functions and $w^+, w^-$ are weight functions corresponding to gains and losses, respectively. The utility functions $u^+$ and $u^-$ are non-decreasing, while the weight functions are continuous, non-decreasing and have the range $[0,1]$ with $w^+(0)=w^-(0)=0$ and $w^+(1)=w^-(1)=1$ . 
Unlike PT, the CPT-value does not violate stochastic dominance.\footnote{In the aforementioned example, increasing $w^-(0.05)$ and $w^+(0.05)$ does not impact outcomes other than those on the extreme, i.e., $-10$ and $180$, respectively. For instance, the weight for outcome $100$ would be $w^+(0.45) - w^+(0.40)$. Thus, CPT formalizes the intuitive notion that humans are sensitive to extreme outcomes and relatively insensitive to intermediate ones.}

\paragraph{Estimation scheme.} 
Let $\hat{p_k}= \frac{1}{n} \sum_{i=1}^n I_{\{X =x_k\}}$. Then, we estimate $V(X)$ as follows:
\begin{align}
 \label{eq:cpt-discrete-est}
\hat V(X) = & \sum_{i=l+1}^{K-1} u^+(x_i) \Big(w^+(\sum_{j=i}^K \hat p_j) - w^+(\sum_{j=i+1}^K \hat p_j) \Big)
- \sum_{i=1}^{l} u^-(x_i) \Big(w^-(\sum_{j=i}^l \hat p_j) - w^-(\sum_{j=i+1}^l \hat p_j) \Big).
\end{align}

\subsubsection*{Main result}
The following proposition presents a sample complexity result for the discrete valued $X$ under the following assumption:\\
\textbf{Assumption (A3).}  The weight functions $w^+(X)$ and $w^-(X)$ are locally Lipschitz continuous, i.e., for any $x$, there exists a $\delta>0$, such that
$$| w^+(x) - w^+(y) | \leq L_x |x-y|, \text{ for all } y \in (x-\delta,x+\delta) $$.\\

We denote $L=\max\{L_k, k=2...K\}$,  where $L_k$ is the Lipschitz constant at $F_k = \sum_{i=k}^K p_i$.

\todoj[inline]{Fix claim to be for $\hat V$}
\begin{proposition}
$A=\max\{x_k, k=1...K\}$, $\delta =\min\{\delta_k\}$, where $\delta_k$ is the half length of the interval that locally Lipschitz of the point $F_k$ holds.
Suppose $\hat{F_k}$ is the empirical estimation of $F_k$, then 
$$P(\left| \sum_{i=1}^K x_k w(\hat{F_k}) - \sum_{i=1}^K x_k w(F_k) \right| >\epsilon) < K\cdot (
e^{-\delta^2\cdot 2n} + e^{-\epsilon^2 2n/(KLA)^2}) $$ 
\end{proposition}
\begin{proof}
 See Section \ref{sec:proofs-discrete}.
\end{proof}

>>>>>>> origin/master
