\documentclass[11pt,letterpaper,english]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\setcitestyle{nocompress}
\bibpunct{(}{)}{;}{a}{}{,}

% \usepackage[nocompress]{cite}
\usepackage{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{authblk}
\usepackage{macros}
\newtheorem{corollary}[theorem]{Corollary}

\begin{document}
\title{Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control}

\author[1]{Prashanth L.A.\thanks{prashla@isr.umd.edu}}
\author[2]{Cheng Jie\thanks{cjie@math.umd.edu}}
\author[3]{Michael Fu\thanks{mfu@isr.umd.edu}}
\author[4]{Steve Marcus\thanks{marcus@umd.edu}}
\author[5]{Csaba Szepesv\'ari\thanks{szepesva@cs.ualberta.ca}}
\affil[1]{\small Institute for Systems Research, University of Maryland}
\affil[2]{\small Department of Mathematics, University of Maryland}
\affil[3]{\small Robert H. Smith School of Business \& Institute for Systems Research,
University of Maryland}
\affil[4]{\small Department of Electrical and Computer Engineering \& Institute for Systems Research,
University of Maryland}
\affil[5]{\small Department of Computing Science,
University of Alberta}

\renewcommand\Authands{ and }

\date{}

\maketitle

\begin{abstract}
Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. 
CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control.
The RL setting presents two particular challenges when CPT is applied: estimating the CPT objective requires estimations of the {\it entire distribution} of the value function and finding a {\it randomized} optimal policy.
The estimation scheme that we propose uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a stochastic shortest path problem. 
We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with  Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy 
optimization scheme.
We provide theoretical convergence guarantees for all the proposed algorithms and also empirically demonstrate the usefulness of our algorithms. 
\end{abstract}

% \keywords{
% Cumulative prospect theory, reinforcement learning, Service systems,  labor optimization, Adaptive labor staffing, Simultaneous
% perturbation stochastic approximation.
% }



\section{Introduction}
\label{sec:introduction}
%Risk-sensitive reinforcement learning (RL) has received a lot of attention recently (cf. \cite{borkar2010learning,borkar2010risk,tamar2012policy,Prashanth13AC}). Previous works consider either an exponential utility formulation (cf. \cite{borkar2010learning}) that implicitly controls the variance or a constrained formulation with explicit constraints on the variance of the cost-to-go (cf. \cite{tamar2012policy,Prashanth13AC}). Another constraint alternative is to bound a coherent risk measure such as Conditional Value-at-Risk (CVaR), while minimizing the usual cost objective (cf. \cite{borkar2010risk,prashanth2014policy}).  

In this paper we consider human-based decision and more specifically reinforcement learning (RL) problems 
where the  reinforcement learning agent controls a system 
to produce outcomes (``rewards'') that are maximally aligned with the preferences of 
one or multiple humans, an arrangement shown on Figure~\ref{fig:flow}.
To support this arrangement, one possibility is to model human preferences 
with the help of some \emph{risk metrics} mapping random returns 
(e.g., the total discounted reward) to some scalar deterministic quantity.
Popular approaches that use such risk metrics include the exponential utility formulation 
(cf. \cite{borkar2010learning}) that implicitly controls the variance.
An alternative is a to consider constrained formulations 
with explicit constraints on the variance of the return (cf. \cite{tamar2012policy,Prashanth13AC}). 
Another constraint alternative is to bound a coherent risk measure such as Conditional Value-at-Risk (CVaR), 
while minimizing the usual cost objective (cf. \cite{borkar2010risk,prashanth2014policy}).  

The risk metrics underlying the above-mentioned works 
are based on the assumption that human decision makers are rational and/or consistent.
While this may hold in certain restricted settings, a large body of literature indicates that humans are neither rational,
\todoc{Add literature supporting this. At least three books:)}
nor consistent (which, in fact, is an unsurprising fact, at least in the experience of the authors of the paper).
In other words, traditional approaches are based on the belief that optimizing the expected utility (EU) is appealing for human subjects. However, there is substantial evidence that this is not case - see 
the survey article \cite{starmer2000developments} and Chapter 4 of the book \cite{quiggin2012generalized}. In particular, the aforementioned references describe the Allais and Ellsberg paradoxes popular among economists for arguing against EU. 
Thus, if the goal is to produce outcomes that are best aligned with human preferences,
an alternative approach is required.
A singularly popular and successful approach in behavioral science and economics
is based on \textit{prospect theory (PT)} \cite{kahneman1979prospect} 
and its later enhancement, the so-called \textit{cumulative prospect theory} (CPT) \cite{tversky1992advances}.
CPT is a rank dependent expected utility model \cite{quiggin2012generalized} that incorporates decision weights to distort probabilities. 
The suitability of this approach to model human decision making (and thus preferences) has been widely documented \cite{prelec1998probability}, \cite{wu1996curvature}, \cite{conlisk1989three}, \cite{camerer1989experimental}, \cite{camerer1992recent}, \cite{harless1992predictions}, \cite{sopher1993test}, \cite{camerer1994violations}, \cite{gonzalez1999shape}, \cite{abdellaoui2000parameter}.
PT/CPT has been applied in a variety of domains, for e.g., healthcare \cite{lenert1999associations},  seismic design \cite{goda2008application}, transportation \cite{gao2010adaptive},\cite{fujii2004drivers}, \cite{ramming2001network}, online auctions \cite{weinberg2005exploring}, insurance  \cite{machina1995non} and finance \cite{barberis1999prospect}, \cite{epstein1989substitution}, \cite{epstein1991substitution}.
\todoc{Again, add some books.
Actually, the examples that I am reading about, e.g., in the book ``Against the Gods'', chapter 13 by Bernstein, suggest
that some big exploits are possible.
And  most examples are about that the framing of a decision problem influences what humans choose.
This is kinda dirty. 
I can totally see how policy makers can exploit this. Is this good?
Can we have some positive (in the sense of being ethical) examples?
E.g., you mentioned the arrangement of shelves in shops or something. Some examples will be badly needed. 
E.g., google on prospect theory decision making gives me 
\url{http://link.springer.com/article/10.1007\%2Fs11424-015-2049-0}.
Perhaps add some to the appendix as background on CPT.
}
%In several real-world systems involving humans, traditional expected utility and risk-sensitive control approaches cannot explain the observed behavior as humans are not rational/consistent. In other words, for a human there do not exist utility functions whose expectation can be maximized nor coherent measures such as CVaR fit well with human preferences. This problem can be alleviated by incorporating distortions in the underlying probabilities of the system. Probabilistic distortions have a long history in behavioral science and economics and a very popular approach comes from \textit{Prospect Theory (PT)} \cite{kahneman1979prospect} and its later enhancement \textit{cumulative prospect theory} (CPT) \cite{tversky1992advances}. 

%\pgfkeyssetvalue{/cfr/soul base dimension}{5pt}
%\begin{tikzpicture}
  %[
  %font=\sffamily\bfseries,
  %line width=0.1*\pgfkeysvalueof{/cfr/soul base dimension},
  %outer sep=0pt,
  %inner sep=0pt,
  %person/.pic={%
    %\node (-head) [circle, minimum size=4*\pgfkeysvalueof{/cfr/soul base dimension}] {};
    %\node (-torso) [below=0pt of -head, rectangle, rounded corners=.4*\pgfkeysvalueof{/cfr/soul base dimension}, minimum width=3.5*\pgfkeysvalueof{/cfr/soul base dimension}, minimum height=6*\pgfkeysvalueof{/cfr/soul base dimension}] {};
    %\node (-right arm) [right=0pt of -torso.north east, yshift=-3.1*\pgfkeysvalueof{/cfr/soul base dimension}, rectangle, minimum width=\pgfkeysvalueof{/cfr/soul base dimension}, minimum height=6*\pgfkeysvalueof{/cfr/soul base dimension}, rounded corners=.4*\pgfkeysvalueof{/cfr/soul base dimension}] {};
    %\node (-left arm) [left=0pt of -torso.north west, yshift=-3.1*\pgfkeysvalueof{/cfr/soul base dimension}, rectangle, minimum width=\pgfkeysvalueof{/cfr/soul base dimension}, minimum height=6*\pgfkeysvalueof{/cfr/soul base dimension}, rounded corners=.4*\pgfkeysvalueof{/cfr/soul base dimension}] {};
    %\node (-left leg) [below=0pt of -torso.south, rectangle, minimum width=1.5*\pgfkeysvalueof{/cfr/soul base dimension}, minimum height=6*\pgfkeysvalueof{/cfr/soul base dimension}, rounded corners=.2*\pgfkeysvalueof{/cfr/soul base dimension}, anchor=north east] {};
    %\node (-right leg) [below=0pt of -torso.south, rectangle, minimum width=1.5*\pgfkeysvalueof{/cfr/soul base dimension}, minimum height=6*\pgfkeysvalueof{/cfr/soul base dimension}, rounded corners=.2*\pgfkeysvalueof{/cfr/soul base dimension}, anchor=north west] {};
    %\draw [rounded corners=.2*\pgfkeysvalueof{/cfr/soul base dimension}] (-right leg.south) -- (-right leg.south west) -- (-left leg.south east) -- (-left leg.south west)  -- (-torso.south west) [rounded corners=.4*\pgfkeysvalueof{/cfr/soul base dimension}] -- (-left arm.south east) -- (-left arm.south west) -- (-left arm.north west) -- (-torso.north west) -- ($(-head.south) - (.5*\pgfkeysvalueof{/cfr/soul base dimension},0)$) arc [start angle=255.5, end angle=-74.5, radius=2*\pgfkeysvalueof{/cfr/soul base dimension}] -- (-torso.north east) -- (-right arm.north east) -- (-right arm.south east)  -- (-right arm.south west) [rounded corners=.2*\pgfkeysvalueof{/cfr/soul base dimension}] -- (-torso.south east)  -- (-right leg.south east) -- (-right leg.south west);
  %}
  %]
  %\thetac (feeling small) [fill=red,blue] {person};
%\end{tikzpicture}

\tikzset{
  pobl/.style={
    inner sep=0pt, outer sep=0pt, fill=#1,
  },
  pobl gron/.style n args={2}{
    pobl=#1, rounded corners=#2,
  },
  pics/person/.style n args={3}{
    code={
      \node (-corff) [pobl=#1, minimum width=.25*#2, minimum height=.375*#2, rotate=#3, pic actions] {};
      \node (-pen) [minimum width=.3*#2, circle, pobl=#1, outer sep=.01*#2, anchor=south, rotate=#3, pic actions] at (-corff.north) {};
      \node (-coes dde) [pobl gron={#1}{1pt}, anchor=north west, minimum width=.12125*#2, minimum height=.25*#2, rotate=#3, pic actions] at (-corff.south west) {};
      \node [pobl=#1, anchor=north, minimum width=.12125*#2, minimum height=.15*#2, rotate=#3, pic actions] at (-coes dde.north) {};
      \node (-coes chwith) [pobl gron={#1}{1pt}, anchor=north east, minimum width=.12125*#2, minimum height=.25*#2, rotate=#3, pic actions] at (-corff.south east) {};
      \node [pobl=#1, anchor=north, minimum width=.12125*#2, minimum height=.15*#2, rotate=#3, pic actions] at (-coes chwith.north) {};
      \node (-braich dde) [pobl gron={#1}{.75pt}, minimum width=.075*#2, minimum height=.325*#2, outer sep=.0064*#2, anchor=north west, rotate=#3, pic actions] at (-corff.north east)  {};
      \node [pobl=#1, minimum width=.05*#2, minimum height=.2*#2, outer sep=.0064*#2, anchor=north west, rotate=#3, pic actions] at (-corff.north east) {};
      \node (-braich chwith) [pobl gron={#1}{.75pt}, minimum width=.075*#2, minimum height=.325*#2, outer sep=.0064*#2, anchor=north east, rotate=#3, pic actions] at (-corff.north west) {};
      \node [pobl=#1, minimum width=.0375*#2, minimum height=.2*#2, outer sep=.0064*#2, anchor=north east, rotate=#3, pic actions] at (-corff.north west) {};
      \node (-fit person) [fit={(-pen.north) (-braich dde.east) (-coes chwith.south) (-braich chwith.west)}] {};
      %\node (-pwy) [below=25pt of -fit person, every pin] {\tikzpictext};
      %\draw [every pin edge] (-fit person) -- (-pwy);
    },
  },
}
\tikzstyle{block} = [draw, fill=white, rectangle,
   minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\begin{figure}[h]
\centering
\scalebox{0.85}{\begin{tikzpicture}[auto, node distance=2cm,>=latex',
    every pin edge/.append style={latex-, shorten <=-2.5pt}
  ]
\node [block,fill=blue!20, minimum height=4em,] (world) {\makecell{\large\bf World}}; 
\node [block,fill=red!20, below=2.5cm of world] (agent) {\makecell{\large\bf Agent}}; 
\node [circle,fill=brown!50!black,right=2cm of world,label=above:{\bf Reward}] (tmp) {};
\coordinate [below left=2.5cm of world] (tmp2);
\coordinate [below right=1cm of agent] (belowagent);

\draw [->,thick] (world)  -| (tmp) |-   (agent);
\draw [->,thick] (agent)  -| (tmp2) |-   (world);
%\draw [->] (agent) --   (world);
\draw pic (person)  [right =3cm of tmp] {person={blue}{50pt}{0}};
\coordinate [right=2.8cm of tmp] (tmp3);
\coordinate [below right=16pt of tmp3] (tmp4);
\draw [->,thick] (tmp) --   (tmp3);
\draw [->,thick,dotted] (tmp4)  |- node[below left=0.1cm and 1cm] {\textbf{CPT model parameters}} (belowagent) -|  (agent.south);
\end{tikzpicture}}
\caption{Operational flow of a human-based decision making system}
\label{fig:flow}
\end{figure}
%\begin{tikzpicture}
  %[
    %every pin edge/.append style={latex-, shorten <=-2.5pt},
  %]

As illustrated in Figure \ref{fig:flow}, we consider a typical RL setting where the environment is unknown, but can be experimented with and propose a CPT based risk metric as the long-term performance objective.  \todoc{This para may need to be rewritten in light that I rewrote the previous one.}
\todoc{Risk measure is a technical term according to wikipedia. Risk metric does not seem to have this technical meaning so I propose using risk metric everywhere.}
CPT is a non-coherent and non-convex measure \todoc{I find it strange to emphasize only these aspects.
What's the goal of announcing these here?}
that is well known among psychologists and economists to be a good model for human decision-making systems, with strong empirical support.
To put it differently, CPT captures well the way humans evaluate outcomes and hence, we offer a CPT-variant of the RL notion of ``value function''. Unlike the regular value function which is the expectation of the return random variable, CPT-value employs a functional that distorts the underlying probabilities. The latter is achieved by fitting CPT model parameters to capture human preferences. The goal then for the learning system is to find a policy that maximizes the CPT-value of ``return''. 

%
%In the realm of sequential decision making under uncertainty, we propose a CPT-based risk measure.  In particular,  
%The current RL solutions cannot handle distortions and my current work is to develop both prediction and control schemes for probabilistically distorted MDPs.
%
%\todop[inline]{Add refs for distorted weights}
%In this paper, we consider a risk measure based on \textit{cumulative prospect theory} (CPT) \cite{tversky1992advances}, which is a non-coherent and non-convex measure that is well known among psychologists and economists to be a good model for human decision-making systems, with strong empirical support. In this paper, we incorporate CPT-based criteria into the classic objective \textit{value function} in a reinforcement learning framework. Intuitively this combination is appealing because it taps into the notion of how humans evaluate outcomes and also, a CPT objective leads to a randomized policy, which although harder to estimate often leads to more intuitively appealing behavior, as illustrated via an example below and also the numerical experiments later.

%%%%%%%%%%%%
In terms of research contributions, this is the first work to combine CPT with RL, and although on the surface it might seem straightforward, in fact there are many research challenges that arise from trying to apply a CPT objective in the RL framework. We outline these challenges as well as our solution approach below. \\
\textbf{Prediction:} In the case of the classic value function, which is an expectation, a simple sample means can be used for estimation, facilitating the use of temporal difference type algorithms. On the other hand, estimating the CPT-value for a given policy is challenging, because 
CPT-value   involves a distribution that is distorted using non-linear weight functions and hence, 
requires that the \textit{entire} distribution to be estimated.\\ 
\textit{Solution:} 
We use a quantile based approach to estimate the CPT-value.
Assuming that the weight functions are \holder continuous with constant $\alpha$,  we establish convergence (asymptotic) of our CPT-value estimate to the true CPT-value and provide a sample complexity result that establishes that  $O\left(\frac1{\epsilon^{2/\alpha}}\right)$ samples are required to be $\epsilon$-close to the CPT-value with high probability. If the weights are Lipschitz (i.e., $\alpha=1$), the resulting sample complexity is the canonical rate $O\left(\frac1{\epsilon^2}\right)$ for Monte carlo type schemes.\\
\textbf{Control:} 
Designing policy optimization algorithms in order to find a \textit{CPT-optimal} policy is challenging because CPT-value is a non-coherent and non-convex risk measure that does not lend itself to dynamic programming approaches such as value/policy iteration due to the lack of a ``Bellman equation''. 
Thus, it is necessary to design new simulation optimization scheme that use sample CPT-value estimates to optimize the policy, which is generally \textit{randomized}. While classic simulation optimization settings usually have a zero mean noise in function evaluations, our setting one has to tradeoff simulation cost with the bias in a manner such that the resulting policy optimization scheme cancels the bias effect and converges. \\
\textit{Solution:} 
We derive the condition that specifies the rate at which the number of samples for predicting the CPT-value should increase such that the bias of CPT-value estimates vanishes asymptotically (see (A3) later).

Using two well-known ideas from the \textit{simulation optimization} literature \cite{fu2015handbook}, we propose three optimization algorithms for solving \eqref{eq:opt-general}. These methods overcome the second and third problems mentioned above and are summarized as follows:\\
\textbf{\textit{Gradient-based methods:}} We propose two algorithms in this class. The first is a gradient algorithm that employs simultaneous perturbation stochastic approximation (SPSA)-based estimates for the gradient of the CPT-value, while the second is a Newton algorithm that also uses SPSA-based estimates of the gradient and also the Hessian. We remark again that, unlike traditional settings for SPSA, our estimates for CPT-value have a non-zero (albeit controlled) bias. We establish that our algorithms converge to a locally CPT-value optimal policy. \\
\textbf{\textit{Gradient-free method:}} We perform a non-trivial adaptation of the algorithm from \cite{chang2013simulation} to devise a globally CPT-value optimizing scheme. The idea is to use a reference model that eventually concentrates on the global minimum and then empirically approximate this reference distribution well-enough. The latter is achieved via natural exponential families in conjunction with Kullback-Leibler (KL) divergence to measure the ``distance'' from the reference distribution. Unlike the setting of \cite{chang2013simulation}, we neither observe the objective function (CPT-value) perfectly nor with zero-mean noise. We establish that our algorithm converges to a globally CPT-value optimal parameter (assuming it exists).


To put things in context, risk-sensitive reinforcement learning problems are generally hard to solve. 
For a discounted MDP, \cite{Sobel82VD} showed that there exists a Bellman equation for the variance of the return, but the underlying Bellman operator is not necessarily monotone. The latter observation rules out policy iteration as a solution approach for variance-constrained MDPs.
Further, even if the transition dynamics are known, \cite{mannor2013algorithmic} show that finding a globally mean-variance optimal policy in a discounted MDP is NP-hard.
For average reward MDPs, \cite{filar1989variance} consider a variance definition that measures how far the instantaneous reward is away from its average, unlike the discounted setting where the variance was of the return r.v. However, for average reward MDPs, \cite{filar1989variance} motivate their variance definition well and then provide NP-hardness results for finding a globally optimal policy with the variance constraint.
CVaR as a risk measure is equally (if not more) complicated as the measure here is a conditional expectation, where the conditioning is on a low probability event. Apart from the hardness of finding CVaR-optimal solutions, estimating CVaR for a fixed policy in a typical RL setting itself is a challenge considering CVaR relates to rare events and to the best of our knowledge, there is no algorithm with theoretical guarantees to estimate CVaR without wasting a lot of samples. There are proposals based on importance sampling (cf. \cite{prashanth2014policy,tamar2014optimizing}), but they lack theoretical guarantees. In contrast, we derive a \textit{provably} sample-efficient scheme for estimating the CPT-value (see next section for a precise definition) for a given policy and use this as the inner loop in a policy optimization schemes that include gradient-based as well as gradient free approaches. Finally, we point out that the CPT-value that we define is a generalization in the sense that one can recover the regular value function and the risk measures such as VaR and CVaR by appropriate choices of a certain weight function used in the definition of CPT value (see the next section for precise details).

\noindent
\textit{\textbf{Closest related work}} is \cite{lin2013stochastic}, where the authors propose a CPT-measure for an abstract MDP setting (see \cite{bertsekas2013abstract}). We differ from \cite{lin2013stochastic} in several ways:
\begin{inparaenum}[\bfseries (i)]
\item We do not have a nested structure for the CPT-value \eqref{eq:cpt-mdp} and this implies the lack of a Bellman equation for our CPT measure; and
\item We do not assume model information, i.e., we operate in a model-free RL setting. Moreover, we develop both estimation and control algorithms with convergence guarantees for the CPT-value function.
\end{inparaenum}

The rest of the paper is organized as follows: 
In Section~\ref{sec:cpt-val}, we introduce the notion of CPT-value of a general random variable $X$ and make a special case illustration when $X$ is the return of a stochastic shortest path problem.
In Section~\ref{sec:cpt-sampling}, we
describe the empirical distribution based scheme for estimating the CPT-value of any random variable. In Sections \ref{sec:1spsa}--\ref{sec:2spsa}, we present the gradient-based algorithms for optimizing the CPT-value. Next, in Section \ref{sec:mras}, we present a gradient-free model-based algorithm for CPT-value optimization in an MDP. We provide the proofs of convergence for all the proposed algorithms in Section~\ref{sec:convergence}.
We present the results from numerical experiments for the CPT-value estimation scheme in Section~\ref{sec:expts} and finally, provide the concluding remarks in Section~\ref{sec:conclusions}.

%%%%%%%%%%%%%5
\section{CPT-value}
\label{sec:cpt-val}
For a real-valued random variable $X$, we first introduce a ``CPT-functional'' that replaces the traditional expectation. Subsequently, we specialize $X$ to be the return of stochastic shortest path problem.
\subsection{General definition}
The CPT-value of the random variable $X$ is a functional defined as
\begin{align}
\C_{u,w}(X) = &\intinfinity w^+(P(u^+(X)>z) dz - \intinfinity w^-(P(u^-(X)>z) dz, \label{eq:cpt-general}
\end{align}
where $u=(u^+,u^-)$, $w=(w^+,w^-)$, $u^+,u^-:\R\rightarrow \R_+$ and $w^+,w^-:[0,1] \rightarrow [0,1]$ are continuous (see assumptions (A1)-(A2) in Section \ref{sec:cpt-sampling} for precise requirements on the $u$ and $w$). For notational convenience, we drop the dependence on $u,w$ and use $\C(X)$ to denote the CPT-value.  

Let us deconstruct the above definition:
 \begin{figure}[h]
   \centering
\tabl{c}{
%\includegraphics[width=3.8in]{utility.png}}
   \scalebox{1.0}{\begin{tikzpicture}
   \begin{axis}[width=11cm,height=6.5cm,legend pos=south east,
          %  grid = major,         
            axis lines=middle,
           % grid style={dashed, gray!30},
            xmin=-5,     % start the diagram at this x-coordinate
            xmax=5,    % end   the diagram at this x-coordinate
            ymin=-4,     % start the diagram at this y-coordinate
            ymax=4,   % end   the diagram at this y-coordinate
           % axis background/.style={fill=white},
            ylabel={\large\bf Utility},
            xlabel={\large\bf Gains},
            x label style={at={(axis cs:4.7,-0.8)}},
            y label style={at={(axis cs:-0.8,4.8)}},
            xticklabels=\empty,
            yticklabels=\empty
            ]
           \addplot[name path=cptplus,domain=0:5, green!35!black, very thick,smooth] 
              {pow(abs(x),0.8)}; 
            \addplot[name path=cptminus,domain=-5:0, red!35!black,very thick,smooth] 
              {-2*pow(abs(x),0.7)}; 
               \addplot[domain=-5:5, blue, thick]           {x}; 
               
               \path[name path=diagplus] (axis cs:0,0) -- (axis cs:5,5);
 			  \path[name path=diagminus] (axis cs:-5,-5) -- (axis cs:0,0);
                \path[name path=xaxisplus] (axis cs:0,0) -- (axis cs:5,0);
                 \path[name path=xaxisminus] (axis cs:-5,0) -- (axis cs:0,0);
                 \path[name path=yaxisplus] (axis cs:0,0) -- (axis cs:0,5);
                 \path[name path=yaxisminus] (axis cs:0,-5) -- (axis cs:0,0);
 
 \addplot [orange!40]  fill between[of= diagminus and yaxisminus];
 \addplot [cyan!20]  fill between[of= diagplus and xaxisplus];
 
 \node at (axis cs:  -3.7,-0.45) {\large\bf Losses};
 \node at (axis cs:  4,2.5) {\large $\bm{u^+}$};
 \node at (axis cs:  -1,-3) {\large $\bm{u^-}$};
   \end{axis}
   \end{tikzpicture}}}\\[1ex]
\caption{Utility function}
\label{fig:u}
\end{figure}

\paragraph{Utility functions:} $u^+, u^-$ are utility functions corresponding to gains ($X \ge 0$) and losses ($X \le 0$), respectively. For example, consider a scenario where one can either earn \$$500$ w.p $1$ or earn \$$1000$ w.p. $0.5$ (and nothing otherwise). The human tendency is to choose the former option of a certain gain. If we flip the situation, i.e., a certain loss of \$$500$ or a loss of \$$1000$ w.p. $0.5$, then humans choose the latter option.  Handling losses and gains separately is a salient feature of CPT, and this addresses the tendency of humans to play safe with gains and take risks with losses - see Fig \ref{fig:u}.  In contrast, the traditional value function makes no such distinction between gains and losses.  

\begin{figure}[h]
\centering
\tabl{c}{
%\includegraphics[width=8cm]{results/Probability_weighting_function.jpg}
  \scalebox{1.0}{\begin{tikzpicture}
  \begin{axis}[width=11cm,height=6.5cm,legend pos=south east,
           grid = major,
           grid style={dashed, gray!30},
           xmin=0,     % start the diagram at this x-coordinate
           xmax=1,    % end   the diagram at this x-coordinate
           ymin=0,     % start the diagram at this y-coordinate
           ymax=1,   % end   the diagram at this y-coordinate
           axis background/.style={fill=white},
           ylabel={\large Weight $\bm{w(p)}$},
           xlabel={\large Probability $\bm{p}$}
           ]
          \addplot[domain=0:1, red, thick,smooth,samples=1500] 
             {pow(x,0.6)/(pow(x,0.6) + pow(1-x,0.6))}; 
             \node at (axis cs:  0.8,0.35) (a1) {\large $\bm{\frac{p^{0.6}}{(p^{0.6}+ (1-p)^{0.6})}}$};           
             \draw[->] (a1) -- (axis cs:  0.7,0.6);
                 \addplot[domain=0:1, blue, thick]           {x};                      
  \end{axis}
  \end{tikzpicture}}\\[1ex]
}
\caption{Weight function}
\label{fig:w}
\end{figure}

\paragraph{Weight functions:} $w^+, w^-$ are functions corresponding to gains and losses, respectively. 
The main idea is that humans deflate high-probabilities and inflate low-probabilities and this is the rationale behind using a weight function in CPT.
For example, humans usually choose a stock that gives one million dollars w.p. $1/10^6$ over one that gives \$$1$ w.p. $1$ and the reverse when signs are flipped. 
Thus the value seen by the human subject is non-linear in the underlying probabilities - an observation with strong empirical evidence that used human subjects (see \cite{tversky1992advances} or $8000$+ papers that follow).  In contrast,the traditional value function is linear in the underlying probabilities. 
As illustrated with $w=w^+=w^-$ in Fig \ref{fig:w}, the weight functions are continuous, non-decreasing and  have the range $[0,1]$ with $w^+(0)=w^-(0)=0$ and $w^+(1)=w^-(1)=1$. 
The authors in \cite{tversky1992advances} recommend $w(p) = \frac{p^{\delta}}{{(p^{\delta}+ (1-p)^{\delta})}^{1/\delta}}$, while \cite{prelec1998probability} recommends $w(p) = \exp(-(-\ln p)^\delta)$, with $0 < \delta <1$ and in both cases, the weight function has the inverted-s shape, which is seen to be a good fit from empirical tests on human subjects - see \cite{conlisk1989three}, \cite{camerer1989experimental}, \cite{camerer1992recent}, \cite{harless1992predictions}, \cite{sopher1993test}, \cite{camerer1994violations}, \cite{gonzalez1999shape}, \cite{abdellaoui2000parameter}.   
Weight functions can explain non-linear probability distortions, as illustrated by the following example: 
\begin{description}
 \item[\textit{[Stock 1]}] This investment results in a gain of \$$10$ with probability (w.p.) $0.1$ and a loss of \$$500$ w.p. $0.9$. The expected return is \$$-449$, but this does not necessarily imply that ``human'' investors' evaluation of the stock is \$$-449$. Instead, it is very likely that the humans evaluate it to a higher value, e.g. \$$-398$ ($=$ gain w.p. $0.2$ and loss w.p. $0.8$).\footnote{See Table 3 in \cite{tversky1992advances} to know why such a human evaluation is likely.}
\item[\textit{[Stock 2]}] loss of \$$10$ w.p. $0.9$, gain \$$500$ w.p. $0.1$. Expected return: \$$41$; Human evaluation: \$$92$ ($=$ loss w.p. $0.8$).
\item[\textit{[Stock 3]}] loss of \$$10$ w.p. 0.1, gain \$$500$ w.p. $0.9$. Expected return: \$$449$; Human evaluation: \$$398$ ($=$ loss w.p. $0.2$). 
\end{description}
These references also include experimental tests on human subjects and conclude that the weight function in non-linear and  inverted-S (such as that in Fig \ref{fig:w}) is a good fit from empirical data. The CPT paper \cite{tversky1992advances} recommends $w(p) = \frac{p^{\delta}}{{(p^{\delta}+ (1-p)^{\delta})}^{1/\delta}}$, while \cite{prelec1998probability} recommends $w(p) = \exp(-(-\ln p)^\delta)$, with $0 < \delta <1$ in both cases and both give the inverted-s shape for weight function. 

\paragraph{Optimization objective:} Suppose the r.v. $X$ is a function of a $d$-dimensional parameter $\theta$. The goal then is to solve the following problem:
\begin{align}
\label{eq:opt-general}
\textrm{Find ~}\theta^* = \argmin_{\theta \in \Theta} \C(X^\theta),
\end{align}
where $\Theta$ is a compact and convex subset of $\R^d$.
The above optimization problem has several applications in RL. For instance, $X$ could be the total reward/discounted reward/average reward r.v. for a fixed policy (given as $\theta$) in the context of a stochastic shortest path/discounted/average reward MDP, respectively. We illustrate one of these applications next.

\subsection{Application: Stochastic Shortest Path}
We consider a stochastic shortest path (SSP) problem with states $\{0,\ldots,\L\}$ and actions $\{1,\ldots,\M\}$. 
An \textit{episode} is a simulated sample path that starts in state $x^0$ and ends in the cost-free absorbing state $0$. 
Let $\theta=\left(\theta^{1},\ldots,\theta^{\L\M}\right)\tr$ be a randomized policy, where $\theta^{i}$ denotes the probability of choosing action $(i\% \M)$ in state $\left \lceil{i/\M}\right \rceil$, with $\sum\limits_{j=(i-1)\M+1}^{i\M} \theta^{j} = 1,$ for $i=1,\ldots,\L$. 
Let $D^\theta(x^0)$ be a random variable (r.v) that denotes the total cost from an episode simulated using policy $\theta$ starting from state $x^0$, i.e.,
$$ D^\theta(x^0) = \sum\limits_{m=0}^{\tau} g(x_m,a_m), $$
where the actions $a_m$ are chosen using  $\theta$ and
$\tau$ is the first passage time to state $0$. 

The traditional RL objective for an SSP is to minimize the expected value $\E (D^\theta(x^0))$ and this can be written as
$$\min_{\theta\in\theta} \intinfinity P(D^\theta(x^0))>z) dz,$$ where $\Theta$ is the set of admissible policies that are \textit{proper}\footnote{A policy $\theta$ is proper if $0$ is recurrent and all other states are transient for the Markov chain underlying $\theta$. It is standard to assume that policies are proper in an SSP setting - cf. \cite{bertsekas1995dynamic}.}.\\
In this paper, we adopt the CPT approach and aim to solve the following problem: 
$$ \min_{\theta \in \theta} \C(D^\theta(x^0)),$$
where the CPT-value function $\C(D^\theta(x^0))$ is defined as
\begin{align}
\C(D^\theta(x^0)) = &\intinfinity w^+(P(u^+(D^\theta(x^0)))>z) dz - \intinfinity w^-(P(u^-(D^\theta(x^0)))>z) dz. \label{eq:cpt-mdp}
\end{align}

\paragraph{Generalization:} It is easy to see that the CPT-value is a generalization of the traditional value function, as a choice of identity map for the weight and utility functions in \eqref{eq:cpt-mdp} makes it the expectation of the total cost $D^\theta)$.  It is also possible to get \eqref{eq:cpt-mdp} to coincide with coherent risk measures (e.g. CVaR) by the appropriate choice of weight functions.

\paragraph{Sensitivity:}
Traditional EU based approaches are sensitive to modeling errors as illustrated in the following example: 
Suppose stock $\cal{A}$ gains \$$10000$ w.p $0.001$ and loses nothing w.p. $0.999$, while stock $\cal B$ surely gains $11$. With the classic value function objective, it is optimal to invest in stock $\cal B$ as it returns $11$,  while $\cal A$ returns $10$ in expectation (assuming utility function to be the identity map). Now, if the gain probability for stock $\cal A$ was $0.002$, then it is no longer optimal to invest in stock $\cal B$ and investing in stock $A$ is optimal.
Notice that a very slight change in the underlying probabilities resulted in a big difference in the investment strategy and a similar observation carries over to a multi-stage scenario (see the house buying example in the numerical experiments section). 
 %A randomized policy that $50$\% in stock $\cal A$ and the rest in a risk-free asset is less sensitive to the error in under-estimating the loss probability. 

Using CPT makes sense because it inflates low probabilities and thus can account for modeling errors, especially considering that model information is unavailable in practice.
Note also that in MDPs with expected utility objective, there exists a deterministic policy that is optimal. However, with CPT-value objective, the optimal policy is \textit{not necessarily} deterministic - See also the organ transplant example on pp. 75-81 of \cite{lin2013stochastic}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CPT-value estimation} 
\label{sec:cpt-sampling}

For the sake of notational simplicity, we let $X$ denote the r.v. $X^\theta$, i.e., where the parameter $\theta$ is assumed to be fixed for the purpose of CPT-value estimation in this section. 

\paragraph{On integrability}
Observe that the first integral in \eqref{eq:cpt-mdp}, i.e., 
\begin{align}
\label{eq:1st-int-cpt}
\int_0^{+\infty} w^+(P(u^+(X)>z)) d z
\end{align}
may diverge even if the first moment of random variable $u^+(X)$ is finite. 
For example, suppose $U$ has the tail distribution function
$$P(U>z)  = \frac{1}{z^2}, z\in [1, +\infty),$$
and $w^+(z)$ takes the form $w(z) = z^{\frac{1}{3}}$. Then, the integral \eqref{eq:1st-int-cpt} with respect to the distorted tail, i.e.,
$$
\int_1^{+\infty} \frac{1}{z^{\frac{2}{3}}} dz
$$
does not even exist. A similar argument applies to the second integral in \eqref{eq:cpt-mdp} as well.

To overcome the above integrability issues, we make different assumptions on the weight and/or utility functions. In particular, we assume that the weight functions $w^+, w^-$ are either 
\begin{inparaenum}[\bfseries (i)]
\item Lipschitz continuous, or
\item \holder continuous, or
\item locally Lipschitz.
\end{inparaenum}
We devise a scheme for estimating \eqref{eq:cpt-mdp} given only samples from $X$ and show that, under each of the aforementioned assumptions, our estimator (presented next) converges almost surely. 
We also provide sample complexity bounds assuming that the utility functions are bounded.

% \subsubsection*{Main results}
% As mentioned earlier, to overcome integrability issues, we make the following assumption:\\[1ex]
% \textbf{Assumption (A1).}  The weight functions $w^+, w^-$ are Lipschitz with common constant $L$, and 
% $u^+(X)$ and $u^-(X)$ both have bounded first moments\\[1ex]
% % If the weight function is not Lipschitz, then the integrals using the distribution of $X$ in CPT-value may not even be finite. 
% We make the following assumptions on the utility functions:\\[1ex]
% \textbf{Assumption (A2).}  The utility functions $u^+(X)$ and $u^-(X)$ are continuous and increasing.\\[1ex]
% \todoj{Check if increasing is necessary}
% \textbf{Assumption (A2').}  In addition to (A2), the utility functions $u^+(X)$ and $u^-(X)$ are bounded above by $M<\infty$.\\[1ex]
% For the convergence rate results below, we require (A2'), while (A2) is sufficient to prove asymptotic convergence.
% 
% The following result shows that the estimate \eqref{eq:cpt-est} converges to the true CPT value almost surely and at the (nearly) canonical Monte Carlo asymptotic rate. 
% \begin{proposition} (\textbf{Asymptotic convergence and rate.})
% \label{thm:asymp-conv}
% Under (A1) and (A2), we have
% \begin{align}
% \widehat V_n(X) \rightarrow \C(X) \text{ a.s. as } n \rightarrow \infty.
% \end{align}
% If we assume (A2'), then we have
% $$
% \limsup_{n\rightarrow \infty} \sqrt{\frac{n}{2 \ln \ln n}} ||\hat{V_n}(X)-\C(X)||_{\infty} 
% \leq  LM \quad \text{a.s.}
% $$
% \end{proposition}
% \begin{proof}
%  See Section \ref{appendix:cpt-est}.
% \end{proof}
% 
% While the above result establishes that \eqref{eq:cpt-est} is an unbiased estimate in the asymptotic sense, it is important to know the rate at which the estimate in \eqref{eq:cpt-est} converges to the CPT-value. 
% The following sample complexity result shows that $O\left(\frac{1}{\epsilon^2}\right)$ number of samples are required to be $\epsilon$-close to the CPT-value in high probability.
% \begin{proposition}(\textbf{Sample Complexity})
% \label{thm:dkw}
% Under (A1) and (A2'), for any $\epsilon, \delta >0$, we have
% \begin{align*}
% P(|\widehat V_n(X) - \C(X)|\le\epsilon) \geq  1 - \delta, \,\,\, \forall n \geq \frac{2 L^2 M^2}{\epsilon^2} ln \frac{4}{\delta}.
% \end{align*}
% \end{proposition}
% \begin{proof}
%  See Section \ref{appendix:cpt-est}.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation scheme for \holder continuous weights}
Recall the H\"{o}lder continuity property first in definition 1:
\begin{definition}
{\textbf{\textit{(H\"{o}lder continuity)}}}
If $0 < \alpha \leq 1$, a function $f \in C([a,b])$ is said to satisfy
a H\"{o}lder condition of order $\alpha$ (or to be H\"{o}lder continuous
of order $\alpha$) if
\[
\sup_{x \neq y} \frac{| f(x) - f(y) |}{| x-y |^{\alpha}} \leq C .
\]
\end{definition}

In order to ensure integrability of the CPT-value \eqref{eq:cpt-mdp}, we make the following assumption:\\[1ex]
\textbf{Assumption (A1).}  
The weight functions $w^+, w^-$ are H\"{o}lder continuous with common order $\alpha$. Further,
$\exists \gamma \le \alpha \text{   s.t,  }$ 
$$\int_0^{+\infty} P^{\gamma} (u^+(X)>z) dz < +\infty \text{ and }\int_0^{+\infty} P^{\gamma} (u^-(X)>z) dz < +\infty$$

The above assumption ensures that the CPT-value as defined by \eqref{eq:cpt-mdp} is finite - see Proposition \ref{prop:Holder-cpt-finite} in Section \ref{sec:holder-proofs} for a formal proof.


\paragraph{Approximating CPT-value using quantiles:}
Let $\xi^+_{\frac{i}{n}}$ denote the $\frac{i}{n}$th quantile of the r.v. $u^+(X)$. Then, it can be seen that (see Proposition \ref{prop:holder-quantile} in Section \ref{sec:holder-proofs})
\begin{align}
\label{eq:holder-quant-motiv}
\lim_{n \rightarrow \infty} \sum_0^{n-1} \xi^+_{\frac{i}{n}} \left(w^+\left(\frac{n-i}{n}\right)- w^+\left(\frac{n-i-1}{n}\right) \right) = \int_0^{+\infty} w^+(P(u^+(X)>z)) dz.
\end{align}

The identical property holds for the pairs $u^-(X)$, $\xi^-_{\frac{i}{n}}, w^-$ , with $\xi^-_{\frac{i}{n}}$ denote the 
$\frac{i}{n}$th quantile of the r.v. $u^-(X)$.

However, we do not know the distribution of $u^+(X)$ or $u^-(X)$ and hence, we develop a procedure that uses order statistics for estimating quantiles, which in turn assists in estimating the CPT-value along the lines of \eqref{eq:holder-quant-motiv}. The estimation scheme is presented in Algorithm \ref{alg:holder-est}.

\todoj[inline]{Need some text here to connect the quantile business to EDFs, i.e., to say that step 5 in Algo 1 is the same as estimating EDFs and then doing empirical integration using EDFs to estiamete CPT-value}

\begin{algorithm}
\caption{CPT-value estimation for \holder continuous weights}
\label{alg:holder-est}
\begin{algorithmic}[1]
\State Simulate $n$ random samples with distribution $X$, sort them and denoted the ordered sample as 
$X_{[1]}, X_{[2]}, \ldots X_{[n]}$.
\State Calculate $u^+(X_{[1]}),\ldots u^+(X_{[n]})$
\State Order the simulated samples and label them as follows: 
$u^+(X_{[1]}),\ldots,u^+(X_{[n]})$.
\State Use $u^+(X_{[i]}), i\in \mathbb{N}\cap (0,n)$ as an approximation for the $\frac{i}{n} th$ quantile of $u^+(X)$, i.e, $\xi_{\frac{i}{n}}, i\in \mathbb{N}\cap (0,n)$.
\State Denote the statistic 
$\overline \C_n^+:=\sum_{i=1}^{n-1} u^+(X_{[i]}) (w^+(\frac{n-i}{n})- w^+(\frac{n-i-1}{n}) )$
\State Repeat the procedure on the sequence $X_{[1]}, X_{[2]}, \ldots X_{[n]}$, with respect to the function $u^-$, 
and denote the statistic $\overline \C_n^-:=\sum_{i=1}^{n-1} u^-(X_{[i]}) (w^-(\frac{n-i}{n})- w^-(\frac{n-i-1}{n}) ) $
\State Return the statistic $\overline \C_n =\overline \C_n^+ - \overline \C_n^-$.
\end{algorithmic}
\end{algorithm}

\subsubsection*{Main results}
We make the following assumptions on the utility functions:\\[1ex]
\textbf{Assumption (A2).}  The utility functions $u^+(X)$ and $u^-(X)$ are continuous and increasing.\\[1ex]
\todoj{Check if increasing is necessary}
\textbf{Assumption (A2').}  In addition to (A2), the utility functions $u^+(X)$ and $u^-(X)$ are bounded above by $M<\infty$.\\[1ex]
For the convergence rate results below, we require (A2'), while (A2) is sufficient to prove asymptotic convergence.

\begin{proposition}(\textbf{Asymptotic convergence.})
\label{prop:holder-asymptotic}
Assume (A1) and also that $F^+(\cdot)$,$F^-(\cdot)$ - the distribution functions of $u^+(X)$, and $u^-(X)$ are Lipschitz continuous with constants $L^+$ and $L^-$, respectively on the interval $(0,+\infty)$, and 
$(-\infty, 0)$ . Then, we have that
\begin{align}
\lim_{n\rightarrow +\infty} 
\overline \C_n
=
\C(X)
 \text{   a.s.,}
\end{align}
where $\overline \C_n$ is as defined in Algorithm \ref{alg:holder-est} and $\C(X)$ as in \eqref{eq:cpt-general}.
\end{proposition}
\begin{proof}
See Section \ref{sec:holder-proofs}.
\end{proof}

While the above result establishes that $\overline \C_n$ is an unbiased estimate in the asymptotic sense, it is important to know the rate at which the estimate $\overline \C_n$ converges to the CPT-value $\C(X)$. 
The following sample complexity result shows that $O\left(\frac{1}{\epsilon^{2/\alpha}}\right)$ number of samples are required to be $\epsilon$-close to the CPT-value in high probability.

% 
% While the above proposition gives an asymptotic guarantee, in the following we provide a sample complexity result for Algorithm \ref{alg:holder-est}.
% For this result, we assume, as in the case of Lipschitz continuous weights, that the utility functions are bounded in addition to (A1').

\begin{proposition}(\textbf{Sample complexity.})
\label{prop:holder-dkw}
Assume (A1) and (A2'). Then, $\forall \epsilon, \delta$, we have
$$
P(\left |\overline \C_n- \C(X) \right| \leq  \epsilon ) > \delta\text{     ,} \forall n \geq \ln(\frac{1}{\delta})\cdot 
\frac{4L^2 M^2}{\epsilon^{2/\alpha}}.$$
\end{proposition}
\begin{proof}
Notice the the following equivalence:
$$\sum_{i=1}^{n-1} u^+(X_{[i]}) (w^+(\frac{n-i}{n}) - w^+(\frac{n-i-1}{n})) =  \int_0^M w^+(1-\hat{F^+_n}(x)) dx, $$
and also,
$$\sum_{i=1}^{n-1} u^-(X_{[i]}) (w^-(\frac{n-i}{n}) - w^-(\frac{n-i-1}{n})) =  \int_0^M w^-(1-\hat{F^-_n}(x)) dx, $$

where $\hat{F^+_n}(x)$ and $\hat{F^-_n}(x)$ is the empirical distribution of $u^+(X)$
and $u^-(X)$, defined as follows:
\begin{align}
{\hat F_n}^+(x)=&\frac{1}{n} \sum_{i=1}^n 1_{(u^+(X_i) \leq x)}, 
{\hat F_n}^-(x)=\frac{1}{n} \sum_{i=1}^n 1_{(u^-(X_i) \leq x)}.
\label{eq:edf}
\end{align}

The main claim follows fromt the equivalence mentioned above together with the well-known DKW inequality.
The detailed proof is available in Section \ref{sec:holder-proofs}.
\end{proof}

\subsubsection*{Special case: Lipschitz continuous weights}
\textbf{Assumption (A1').}  The weight functions $w^+, w^-$ are Lipschitz with common constant $L$, and 
$u^+(X)$ and $u^-(X)$ both have bounded first moments.\\[1ex]

\begin{corollary}[Lipschitz case]
Assume (A1') and (A2). Then, we have that 
$$\lim_{n\rightarrow +\infty} \overline \C_n = \C(X) \text{   a.s.}$$

In addition, if we assume (A2'), we have 
$$
P(\left |\overline \C_n- \C(X) \right| \leq  \epsilon ) > \delta\text{     ,} \forall n \geq \ln(\frac{1}{\delta})\cdot 
\frac{4L^2 M^2}{\epsilon^{2}}.
$$
\end{corollary}
\begin{proof}
 Setting $\alpha=\gamma=1$ in the proof of Proposition \ref{prop:Holder-cpt-finite}, it is easy to see that the CPT-value \eqref{eq:cpt-mdp} is finite. Thus, the claims regarding asymptotic convergence and sample complexity as special cases of Proposition \ref{prop:holder-asymptotic}--\ref{prop:holder-dkw}, with $\alpha=1$. 
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation scheme for locally Lipschitz weights and discrete $X$}
\todop[inline]{Would be better if the background been put to the introduction part}
\input{discrete-est}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradient-based algorithm for CPT optimization (CPT-SPSA)}
\label{sec:1spsa}

\begin{figure}[h]
\centering
\tikzstyle{block} = [draw, fill=white, rectangle,
   minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
\scalebox{0.85}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {\large$\bm{\theta_n}$};
\node [sum, fill=blue!20,above right=0.6cm of theta, xshift=1cm] (perturb) {\large$\bm{+}$};
\node [sum,fill=red!20, below right=0.6cm of theta, xshift=1cm] (perturb1) {\large$\bm{-}$};
\node [above=0.5cm of perturb] (noise) {\large$\bm{\delta_n \Delta_n}$};
\node [below=0.5cm of perturb1] (noise1) {\large$\bm{\delta_n \Delta_n}$};    
\node [block,fill=blue!20, right=2.5cm of perturb,label=above:{\color{bleu2}\bf Prediction}, minimum height=4em,] (psim) {\makecell{\large\bf CPT-value estimate\\[1ex] \large\bf for $\bm{\theta_n+\delta_n \Delta_n}$}}; 
\node [block,fill=red!20, right=2.5cm of perturb1] (sim) {\makecell{\large\bf CPT-value estimate\\[1ex] \large\bf for $\bm{\theta_n-\delta_n \Delta_n}$}}; 
\node [block, fill=green!20,below right=2cm of psim,label=above:{\color{bleu2}\bf Control}, minimum height=8em, yshift=2.5cm,text width=3.2cm] (update) {\large\bf{Gradient descent }\\[2ex]\large\bf{~~~using SPSA}};
\node [right=0.7cm of update] (thetanext) {\large$\bm{\theta_{n+1}}$};

\draw [->] (perturb) -- node[above] {\textbf{Obtain}} node[below] {$\bm{m_n}$ \textbf{samples}}  (psim);
\draw [->] (perturb1) -- node[above] {\textbf{Obtain}} node[below] {$\bm{m_n}$ \textbf{samples}}  (sim);
\draw [->] (noise) -- (perturb);
\draw [->] (noise1) -- (perturb1);
\draw [->] (psim) -- %node {$\hat J^{\theta(t)+p_1(t)}(x_0)$}
(update.150);
\draw [->] (sim) --  %node {$\hat J^{\theta(t)+p_2(t)}(x_0)$} 
(update.205);
\draw [->] (update) -- (thetanext);
\draw [->] (theta) --   (perturb);
\draw [->] (theta) --   (perturb1);
\end{tikzpicture}}
\caption{Overall flow of CPT-SPSA-G.}
\label{fig:algorithm-flow}
\end{figure}

\subsection{Gradient estimation} 
Given that we operate in a learning setting and only have biased estimates of the CPT-value from Algorithm \ref{alg:holder-est}, we require a simulation optimization scheme that estimates $\nabla \C(X^\theta)$.  
Simultaneous perturbation methods are a general class of stochastic gradient schemes that optimize a function given only noisy sample values - see \cite{Bhatnagar13SR} for textbook introduction. SPSA is a well-known scheme that estimates the gradient using two sample values. In our context, at any iteration $n$ of CPT-SPSA-G, with parameter $\theta_n$, the gradient $\nabla \C(X^{\theta_n})$ is estimated as follows: For any  $i=1,\ldots,d$,
$$\widehat \nabla_{i} \C(X^\theta) = \dfrac{\overline \C_n^{\theta_n+\delta_n \Delta_n} - \overline \C_n^{\theta_n-\delta_n \Delta_n}}{2 \delta_n \Delta_n^{i}},$$
where $\delta_n$ is a positive scalar that satisfies (A3) below, $\Delta_n = \left( \Delta_n^{1},\ldots,\Delta_n^{\L\M}\right)\tr$, where $\{\Delta_n^{i}, i=1,\ldots,\L\M\}$, $n=1,2,\ldots$ are i.i.d. Rademacher, independent of $\theta_0,\ldots,\theta_n$ and $\overline \C_n^{\theta_n+\delta_n \Delta_n}$ (resp. $\overline \C_n^{\theta_n+\delta_n \Delta_n}$) denotes the CPT-value estimate that uses $m_n$ samples of the r.v. $X^{\theta_n+\delta_n \Delta_n}$ (resp. $\overline X^{\theta_n-\delta_n \Delta_n}$).
%From the asymptotic mean square analysis that we present later, it is optimal to set $\delta_n = \delta_0/n^{0.16}$.
The (asymptotic) unbiasedness of the gradient estimate is proven in Lemma \ref{lemma:1spsa-bias}.

This idea of using two-point feedback for estimating the gradient has been employed in various settings. Machine learning applications include bandit/stochastic convex optimization - cf. 
\cite{hazan2015online}, \cite{duchi2013optimal}. However, the idea applies to non-convex functions as well - cf. \cite{spall2005introduction}, \cite{Bhatnagar13SR}.


\subsection{Update rule} We incrementally update the parameter $\theta$ in the descent direction as follows: For every state $i=1,\ldots,\L\M$,
\begin{align}
\theta^{i}_{n+1} = \Gamma_{i}\left(\theta^{i}_n - \gamma_n  \widehat \nabla_{i} \C(X^{\theta_n})\right),
\label{eq:theta-update}
\end{align}
where  $\gamma_n$ is a step-size chosen to satisfy (A3) below and
$\Gamma=\left(\Gamma_{1},\ldots,\Gamma_{d}\right)$ is an operator that ensures that the update \eqref{eq:theta-update} stays bounded within a compact and convex set $\Theta$. 
Fig. \ref{fig:algorithm-flow} illustrates the overall flow of the gradient algorithm based on SPSA, while Algorithm \ref{alg:1spsa}  presents the pseudocode.  


%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
\algblock{PEval}{EndPEval}
\algnewcommand\algorithmicPEval{\textbf{\em CPT-value Estimation (Trajectory 1)}}
 \algnewcommand\algorithmicendPEval{}
\algrenewtext{PEval}[1]{\algorithmicPEval\ #1}
\algrenewtext{EndPEval}{\algorithmicendPEval}

\algblock{PEvalPrime}{EndPEvalPrime}
\algnewcommand\algorithmicPEvalPrime{\textbf{\em CPT-value Estimation (Trajectory 2)}}
 \algnewcommand\algorithmicendPEvalPrime{}
\algrenewtext{PEvalPrime}[1]{\algorithmicPEvalPrime\ #1}
\algrenewtext{EndPEvalPrime}{\algorithmicendPEvalPrime}

\algblock{PImp}{EndPImp}
\algnewcommand\algorithmicPImp{\textbf{\em Gradient descent}}
 \algnewcommand\algorithmicendPImp{}
\algrenewtext{PImp}[1]{\algorithmicPImp\ #1}
\algrenewtext{EndPImp}{\algorithmicendPImp}

\algtext*{EndPEval}
\algtext*{EndPEvalPrime}
\algtext*{EndPImp}
%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\begin{algorithmic}
    \State {\bf Input:}  initial parameter $\theta_0$, perturbation constants $\delta_n>0$, sample size $\{m_n\}$, step-sizes $\{\gamma_n\}$, operator $\Gamma$.
\For{$n = 0,1,2,\ldots$}	
	\State Generate $\{\Delta_n^i, i=1,\ldots,\L\M\}$ using Rademacher distribution, independent of $\{\Delta_m, m=0,1,\ldots,n-1\}$
	\PEval
	    \State Simulate $m_n$ samples using parameter $(\theta_n+\delta_n \Delta_n)$
	    \State Obtain CPT-value estimate $\overline \C_n^{\theta_n+\delta_n \Delta_n}$
	    \EndPEval
	    \PEvalPrime
  	    \State Simulate $m_n$ samples using parameter $\theta_n-\delta_n \Delta_n$
	    \State Obtain CPT-value estimate $\overline \C_n^{\theta_n-\delta_n \Delta_n}$
	    \EndPEvalPrime
	    \PImp
		\State $\theta^{i}_{n+1} = \Gamma_i\left(\theta^{i}_n - \gamma_n \widehat \nabla_{i} \C(X^{\theta_n})\right)$
		\EndPImp
\EndFor
\State {\bf Return} $\theta_n$
\end{algorithmic}
\caption{Structure of CPT-SPSA-G algorithm.}
\label{alg:1spsa}
\end{algorithm}

 \begin{figure}
    \centering
     \begin{tabular}{cc}
\begin{subfigure}[b]{0.5\textwidth}
\scalebox{0.6}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {$\boldsymbol{\theta}$};
\node [block, fill=blue!20,right=0.6cm of theta,align=center] (sample) {\makecell{\textbf{Measurement}\\\textbf{ Oracle}}}; 
\node [right=0.6cm of sample] (end) {$\boldsymbol{\mathbf{f(\theta) + \xi}}$};
\node [ above right= 0.6cm of end] (bias) {\textbf{Zero mean}};
\draw [->] (theta) --  (sample);
\draw [->] (sample) -- (end);
\path [darkgreen,->] (bias) edge [bend left] (end);
\end{tikzpicture}}
\caption{Simulation optimization}
\end{subfigure}
&
\begin{subfigure}[b]{0.5\textwidth}
\scalebox{0.6}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {$\boldsymbol{\theta, \epsilon}$};
\node [block, fill=blue!20,right=0.6cm of theta,align=center] (sample) {\makecell{\textbf{CPT}\\\textbf{ Estimator}}}; 
\node [right=0.6cm of sample] (end) {$\boldsymbol{\mathbf{\C(X^\theta) + \epsilon}}$};
\node [ above right= 0.6cm of end] (bias) {\textbf{Controlled bias}};
\draw [->] (theta) --  (sample);
\draw [->] (sample) -- (end);
\path [red,->] (bias) edge [bend left] (end);
\end{tikzpicture}}
\caption{CPT-value optimization}
\end{subfigure}
\end{tabular}
\caption{Illustration of difference between classic simulation optimization and CPT-value optimiziation settings}
\label{fig:opt-diff}
\end{figure}

\paragraph{On the number of samples $m_n$ per iteration:}
As illustrated in Figure \ref{fig:opt-diff}, the CPT-value estimation scheme is biased, i.e., providing samples with parameter $\theta_n$ at instant $n$, we obtain its CPT-value estimate as $\C(X^{\theta_n}) + \epsilon_n^\theta$ with $\epsilon_n^\theta$ denoting the bias. The bias can be controlled by increasing the number of samples $m_n$ in each iteration of CPT-SPSA (see Algorithm \ref{alg:1spsa}). This is unlike classic simulation optimization settings where one only sees function evaluations with zero mean noise and there is no question of deciding on $m_n$ to control the bias as we have in our setting.

To motivate the choice for $m_n$, we first rewrite the update rule \eqref{eq:theta-update} as follows:
\begin{align*}
\theta^{i}_{n+1}  = & \Gamma_{i}\bigg( \theta^{i}_n -  \gamma_n \bigg( \frac{\C(X^{\theta_n +\delta_n\Delta_n}) - \C(X^{\theta_n-\delta_n\Delta_n})}{2\delta_n\Delta_n^{i}}\bigg) + \underbrace{\frac{(\epsilon_n^{\theta_n +\delta_n\Delta_n} - \epsilon_n^{\theta_n-\delta_n\Delta_n})}{2\delta_n\Delta_n^{i}}}_{\kappa_n}\bigg).
\end{align*}
Let $\zeta_n = \sum_{l = 0}^{n} \gamma_l \kappa_{l}$. Then, a critical requirement that allows us to ignore the bias term $\zeta_n$ is the following condition (see Lemma 1 in Chapter 2 of \cite{borkar2008stochastic}): 
$$\sup_{l\ge0} \left (\zeta_{n+l} - \zeta_n \right) \rightarrow 0 \text{ as } n\rightarrow\infty.$$ 
While Theorems \ref{prop:holder-asymptotic}--\ref{prop:holder-dkw} show that the bias $\epsilon^\theta$ is bounded above, to establish convergence of the policy gradient recursion \eqref{eq:theta-update}, we increase the number of samples $m_n$ so that the bias vanishes asymptotically.  Assumption (A3) provides a condition on the rate at which $m_n$ has to increase.

\noindent\textbf{Assumption (A3).}  The step-sizes $\gamma_n$ and the perturbation constants 
$\delta_n$ are positive $\forall n$ and satisfy
\begin{align*}
\gamma_n, \delta_n \rightarrow 0, \frac{1}{m_n^{\alpha/2}\delta_n}\rightarrow 0,  \sum_n \gamma_n=\infty \text{ and } \sum_n \frac{\gamma_n^2}{\delta_n^2}<\infty. 
\end{align*}
While the conditions on $\gamma_n$ and $\delta_n$ are standard for SPSA-based algorithms, the condition on $m_n$ is motivated by the earlier discussion. 
A simple choice that satisfies the above conditions is $\gamma_n = a_0/n$, $m_n = m_0 n^\nu$ and $\delta_n = \delta_0/{n^\gamma}$, for some $\nu, \gamma >0$ with $\gamma > \nu\alpha/2$.

\subsection{Convergence result}
\begin{theorem}
\label{thm:1spsa-conv}
Assume (A1)-(A3).
Consider the  ordinary differential equation (ODE): 
$$\dot\theta^{i}_t = \check\Gamma_{i}\left(- \nabla \C(X^{\theta^{i}_t})\right), \text{ for }i=1,\dots,d,$$ 
where 
$$\check\Gamma_{i}(f(\theta)) := \lim\limits_{\alpha \downarrow 0} \frac{\Gamma_{i}(\theta + \alpha f(\theta)) - \theta}{\alpha}, \textrm{ for any continuous }f(\cdot).$$
 Let $\K = \{\theta \mid \check\Gamma_{i} \left(\nabla_i V^{\theta}(x^0)\right)=0, \forall i=1,\ldots,d\}$. Then,
$$\theta_n \rightarrow \K \text{ a.s. as } n\rightarrow \infty.$$
\end{theorem}
\begin{proof}
 See Section \ref{appendix:1spsa}.
\end{proof}

%See Theorem \ref{thm:1spsa-asymp-normal} in Appendix \ref{appendix:1spsa} for a central limit theorem result, which shows that $n^{\beta/2}(\theta_n - \theta^*)$ is asymptotically normal.  


%%%%%%%%%%
\section{Newton algorithm for CPT-value optimization (CPT-SPSA-N)}
\label{sec:2spsa}
\subsection{Need for second-order methods}
While stochastic gradient descent methods are useful in minimizing the CPT-value given biased estimates, they are sensitive to the choice of the step-size sequence $\{\gamma_n\}$.  In particular, for a step-size choice $\gamma_n = \gamma_0/n$, if $a_0$ is not chosen to be greater than $1/3 \lambda_{min}(\nabla^2 \C(X^{\theta^*}))$, then the optimum rate of convergence is not achieved. Here $\lambda_{\min}$ denotes the minimum eigenvalue, while $\theta^*\in \K$ (see Theorem \ref{thm:1spsa-conv}). A standard approach to overcome this step-size dependency is to use iterate averaging, suggested independently by Polyak \cite{polyak1992acceleration} and Ruppert \cite{ruppert1991stochastic}. The idea is to use larger step-sizes $\gamma_n = 1/n^\varsigma$, where $\varsigma \in (1/2,1)$, and then combine it with averaging of the iterates. However, it is well known  that iterate averaging is optimal only in an asymptotic sense, while finite-time bounds show that the initial condition is not forgotten sub-exponentially fast (see 
Theorem 2.2 in \cite{fathi2013transport}). Thus, it is optimal to average iterates only 
after a sufficient number of iterations have passed and all the iterates are very close to the optimum. However, the latter situation serves as a stopping condition in practice.

An alternative approach is to employ step-sizes of the form $\gamma_n = (a_0/n) M_n$, where $M_n$ converges to $\left(\nabla^2 \C(X^{\theta^*})\right)^{-1}$, i.e., the inverse of the Hessian of the CPT-value at the optimum $\theta^*$. Such a scheme gets rid of the step-size dependency (one can set $a_0=1$) and still obtains optimal convergence rates. This is the motivation behind having a second-order optimization scheme.

\subsection{Gradient and Hessian estimation}
We estimate the Hessian of the CPT-value function using the scheme suggested by \cite{bhatnagar2015simultaneous}. As in the case of the first-order method, we use Rademacher random variables to simultaneously perturb all the coordinates. However, in this case, we require three system trajectories with corresponding  parameters $\theta_n+\delta_n(\Delta_n+\widehat\Delta_n)$, $\theta_n-\delta_n(\Delta_n+\widehat\Delta_n)$ and $\theta_n$, where $\{\Delta_n^i, \widehat\Delta_n^i, i=1,\ldots,d\}$ are i.i.d. Rademacher and independent of $\theta_0,\ldots,\theta_n$. Using the CPT-value estimates for the aforementioned  parameters, we estimate the Hessian and the gradient of the CPT-value function as follows: For $i,j=1,\ldots,d$, set
\begin{align*}
&\widehat \nabla_{i} \C(X_n^{\theta_n})=\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n} - \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n}}{2\delta_n \Delta_n^{i}},\\ 
&\widehat H_n^{i,j}=\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n} - 2\widehat V_n^{\theta_n}(x^0)}{\delta_n^2 \Delta_n^{i}\widehat\Delta_n^{j}}.
\end{align*}
Further, set  $\widehat H_n^{i,j} = \widehat H_n^{j, i}$, for $i,j=1,\ldots,d$.
Notice that the above estimates require three samples, while the second-order SPSA algorithm proposed first in \cite{spall2000adaptive} required four.
%
Both the gradient estimate $\widehat \nabla V_n^{\theta_n}(x^0)$ and the Hessian estimate $\widehat{H_n}$ can be shown to be an $O(\delta_n^2)$ term away from the true gradient $\nabla V^\theta_n(x^0)$ and Hessian $\nabla^2  V^\theta_n(x^0)$, respectively (see Lemmas \ref{lemma:2spsa-bias}--\ref{lemma:2spsa-grad}).

%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
\algblock{PEvalPrimeDouble}{EndPEvalPrimeDouble}
\algnewcommand\algorithmicPEvalPrimeDouble{\textbf{\em CPT-value Estimation (Trajectory 3)}}
 \algnewcommand\algorithmicendPEvalPrimeDouble{}
\algrenewtext{PEvalPrimeDouble}[1]{\algorithmicPEvalPrimeDouble\ #1}
\algrenewtext{EndPEvalPrimeDouble}{\algorithmicendPEvalPrimeDouble}
\algtext*{EndPEvalPrimeDouble}

\algblock{PImpNewton}{EndPImpNewton}
\algnewcommand\algorithmicPImpNewton{\textbf{\em Newton step}}
 \algnewcommand\algorithmicendPImpNewton{}
\algrenewtext{PImpNewton}[1]{\algorithmicPImpNewton\ #1}
\algrenewtext{EndPImpNewton}{\algorithmicendPImpNewton}

\algtext*{EndPImpNewton}

%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[t]
\begin{algorithmic}
\State {\bf Input:} initial parameter $\theta_0$, perturbation constants $\delta_n>0$, trajectory lengths $\{m_n\}$, step-sizes $\{\gamma_n, \xi_n\}$. 
\For{$n = 0,1,2,\ldots$}	
	\State Generate $\{\Delta_n^{i}, \widehat\Delta_n^{i}, i=1,\ldots,d\}$ using Rademacher distribution, independent of $\{\Delta_m, \widehat \Delta_m, m=0,1,\ldots,n-1\}$
	\PEval
	    \State Simulate $m_n$ samples  using parameter $(\theta_n+\delta_n (\Delta_n + \hat \Delta_n))$
	    \State Obtain CPT-value estimate $\widehat V_n^{(\theta_n+\delta_n (\Delta_n+\hat \Delta_n))}(x^0)$ using Algorithm \ref{alg:holder-est}
	    \EndPEval
	    \PEvalPrime
  	    \State Simulate $m_n$ samples using parameter $(\theta_n-\delta_n (\Delta_n + \hat \Delta_n))$
	    \State Obtain CPT-value estimate $\widehat V_n^{\theta_n-\delta_n (\Delta_n+\hat\Delta_n)}(x^0))$ Algorithm \ref{alg:holder-est}
	    \EndPEvalPrime
	    	    \PEvalPrimeDouble
  	    \State Simulate $m_n$ samples using parameter $\theta_n$
	    \State Obtain CPT-value estimate $\widehat V_n^{\theta_n}(x^0))$ using Algorithm \ref{alg:holder-est}
	    \EndPEvalPrimeDouble
	    \PImpNewton
		\State Gradient estimate $\widehat \nabla_{i} \C(X^\theta_n)\quad=\quad\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n} - \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n}}{2\delta_n \Delta_n^{i}}$
        \State Hessian estimate $\widehat H_n\quad=\quad\dfrac{\overline \C_n^{\theta_n+\delta_n(\Delta_n+\widehat\Delta_n} + \overline \C_n^{\theta_n-\delta_n(\Delta_n+\widehat\Delta_n} - 2\widehat \nabla_{i} \C(X^\theta_n)}{\delta_n^2 \Delta_n^{i}\widehat\Delta_n^j}$

		\State Update the parameter and Hessian according to \eqref{eq:2spsa}--\eqref{eq:2spsa-H}
		\EndPImpNewton
\EndFor
\State {\bf Return} $\theta_n$
\end{algorithmic}
\caption{Structure of CPT-SPSA-N algorithm.}
\label{alg:structure-2}
\end{algorithm}

\subsection{Update rule}
We update the parameter incrementally using a Newton decrement as follows: For $i=1,\ldots,d$,
\begin{align}
\label{eq:2spsa}
% \theta_{n+1} =& \theta_{(1-\xi)\Theta}(\theta_n - \gamma_n \Upsilon(\overline H_n)^{-1} \widehat\nabla V^\theta_n(x^0)), \\
\theta^{i}_{n+1} =& \Gamma_{i}\left(\theta^{i}_n - \gamma_n \sum_{j=1}^{\L\M} M_n^{i,j} \widehat \nabla_{j} \C(X^\theta_n)\right), \\
\overline H_n = & (1-\xi_n) \overline H_{n-1} + \xi_n \widehat H_n,\label{eq:2spsa-H}
\end{align}
where $\xi_n$ is a step-size sequence that satisfies 
$\sum_{n} \xi_n = \infty, \sum_n \xi_n^2 < \infty$ and $\frac{\gamma_n}{\xi_n}\rightarrow 0$ as $n\rightarrow \infty$. These conditions on $\xi_n$ ensure that the updates to $\overline H_n$ proceed on a timescale that is faster than that of $\theta_n$ in \eqref{eq:2spsa} - see \cite[Chapter 6]{borkar2008stochastic}.
Further, $\Gamma$ is a projection operator as in CPT-SPSA-G and  $M_n = \Upsilon(\overline H_n)^{-1}$.
% ,  $\widehat\nabla V^\theta_n(x^0)$ is an estimate of the gradient of the CPT-value function and $\widehat H_n$ and $\overline H_n$ denote the Hessian estimate and its smooth counterpart, respectively. 
Notice that we invert $\overline H_n$ in each iteration, and to ensure that this inversion is feasible (so that the $\theta$-recursion descends), we project $\overline H_n$ onto the set of positive definite matrices using the operator $\Upsilon$. The operator has to be such that asymptotically $\Upsilon(\overline H_n)$ should be the same as $\overline H_n$ (since the latter would converge to the true Hessian), while ensuring inversion is feasible in the initial iterations.  The assumption below makes these requirements precise.\\[1ex]
\textbf{Assumption (A4).}  For any $\{A_n\}$ and $\{B_n\}$,
${\displaystyle \lim_{n\rightarrow \infty} \left\| A_n-B_n \right\|}= 0 \Rightarrow {\displaystyle \lim_{n\rightarrow \infty} \parallel \Upsilon(A_n)- \Upsilon(B_n) \parallel}= 0$. Further, for any $\{C_n\}$  with
${\displaystyle \sup_n \parallel C_n\parallel}<\infty$,
${\displaystyle \sup_n \left(\parallel \Upsilon(C_n)\parallel + \parallel \{\Upsilon(C_n)\}^{-1} \parallel\right) < \infty}$
as well.
\\[0.5ex]
%A simple way to define $\Upsilon(\overline H_n)$ is to first perform an eigen-decomposition of $\overline H_n$, followed by projecting all the eigen values onto the positive side (see \cite{gill1981practical} for a similar operator). 
A simple way to ensure the above is to have $\Upsilon(\overline H_n)$ as a diagonal matrix and then add a positive scalar $\delta_n$ to the diagonal elements so as to ensure invertibility  - see \cite{gill1981practical}, \cite{spall2000adaptive} for a similar operator.
%- this choice satisfies requirement (ii) in Theorem \ref{thm:2spsa} presented below.

%We next specify how the gradient $\widehat \nabla_i V^\theta_n(x^0)$ and Hessian $\widehat H_n$ estimates are obtained using SPSA.
The overall flow on CPT-SPSA-N is similar to Fig. \ref{fig:algorithm-flow}, except that three system trajectories with a different perturbation sequence are used. Algorithm \ref{alg:structure-2} presents the pseudocode.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence result}
\begin{theorem}
\label{thm:2spsa}
Assume (A1)-(A4). 
Consider the ODE: 
$$
\dot\theta^{i}_t = \check\Gamma_{i}\left( - \Upsilon(\nabla^2 \C(X^{\theta_t}))^{-1} \nabla \C(X^{\theta^{i}_t}) \right), \text { for }i=1,\dots,d,$$
where 
$\bar\Gamma_{i}$ is as defined in Theorem \ref{thm:1spsa-conv}. Let $\K = \{\theta \in \Theta \mid
\nabla \C(X^{\theta^{i}})  \check\Gamma_{i}\left(-\Upsilon(\nabla^2 \C(X^{\theta}))^{-1} \nabla \C(X^{\theta^{i}})\right)
=0, \forall i=1,\ldots,d\}$. Then,
we have
$$\theta_n \rightarrow \K  \text{~~ a.s. as } n\rightarrow \infty.$$ 
\end{theorem}
\begin{proof}
 See Section \ref{appendix:2spsa}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient-free CPT-value optimization algorithm (GF-CPT)}
\label{sec:mras}
We perform a non-trivial adaptation of the algorithm from \cite{chang2013simulation} to our setting of optimizing CPT-value in MDPs.
We require that there exists a unique global optimum $\theta^*$ for the problem $\min_{\theta \in \Theta} \C(X^\theta).$

\subsection{Basic algorithm}
To illustrate the main idea in the algorithm, assume we know the form of $\C(X^\theta)$. Then, the idea is to generate a sequence of reference distributions $g_k(\theta)$ on the parameter space $\Theta$, such that it eventually concentrates on the global optimum $\theta^*$. One simple way, suggested in Chapter 4 of \cite{chang2013simulation} is
\begin{equation}
\label{eqn:naive}
g_{k}(\theta)=
\frac{\mathcal{H}(\C(X^\theta))g_{k-1}(\theta)}
{\int_{\theta}\mathcal{H}(\C(X^{\theta'}))g_{k-1}(\theta')\nu(d\theta')},
~~\forall\, \theta \in \theta,
\end{equation}
where $\nu$ is the Lebesgue/counting measure on $\theta$ and $\mathcal{H}$ is a strictly decreasing function. The above construction for $g_k$'s assigns more weight to policies having lower CPT-values and it is easy to show that $g_k$ converges to a point-mass concentrated at $\theta^*$.

Next, consider a setting where one can obtain the CPT-value $\C(X^\theta)$ (without any noise) for any policy $\theta$. In this case, we consider a family of parameterized distributions, say $\{f(\cdot,\eta),\,\eta\in \C \}$ and incrementally update the distribution parameter $\eta$ such that it minimizes the following KL divergence:
\begin{equation}\label{eqn:kl}
\mathcal{D}(g_k,f(\cdot,\eta)):=E_{g_k}\left[\ln \frac{g_{k}(\mathcal{R}(\theta))}{f(\mathcal{R}(\theta),\eta)}\right]=\int_{\theta} \ln \frac{g_{k}(\theta)}{f(\theta,\eta)}g_{k}(\theta)\nu(d\theta), \nonumber
\end{equation}
where $\mathcal{R}(\theta)$ is a random vector taking values in the policy space $\theta$. 
%Through \ e intuition for the above is that we project the reference distributions $g_k$ onto the family $\{f(\cdot,\eta),\,\eta\in \C \}$.
An algorithm to optimize CPT-value in this \textit{noise-less} setting would perform the following update for the parameter $\eta_n$:
\begin{equation}
\label{eqn:interior}
\eta_{n+1} \in \argmin_{\eta \in \C}
E_{\eta_n}\left[\frac{[\mathcal{H}(\C(X^{\mathcal{R}(\theta)}))]^{n}}
{f(\mathcal{R}(\theta),\eta_{n})}
\ln f(\mathcal{R}(\theta),\eta)\right],
\end{equation}
where $E_{\eta_n}[\C(X^{\mathcal{R}(\theta)})]=\int_{\theta} V^{\theta}(x^0)f(\theta,\eta_n)\nu(d\theta).$



%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
\algblock{Candidate}{EndCandidate}
\algnewcommand\algorithmicCandidate{\textbf{\em Candidate Policies}}
 \algnewcommand\algorithmicendCandidate{}
\algrenewtext{Candidate}[1]{\algorithmicCandidate\ #1}
\algrenewtext{EndCandidate}{\algorithmicendCandidate}
%%%%%%%%%%%%%%%%%%%
\algblock{Estimation}{EndEstimation}
\algnewcommand\algorithmicEstimation{\textbf{\em CPT-value Estimation}}
 \algnewcommand\algorithmicendEstimation{}
\algrenewtext{Estimation}[1]{\algorithmicEstimation\ #1}
\algrenewtext{EndEstimation}{\algorithmicendEstimation}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\algblock{Elite}{EndElite}
\algnewcommand\algorithmicElite{\textbf{\em Elite Sampling}}
 \algnewcommand\algorithmicendElite{}
\algrenewtext{Elite}[1]{\algorithmicElite\ #1}
\algrenewtext{EndElite}{\algorithmicendElite}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\algblock{Thresholding}{EndThresholding}
\algnewcommand\algorithmicThresholding{\textbf{\em Thresholding}}
 \algnewcommand\algorithmicendThresholding{}
\algrenewtext{Thresholding}[1]{\algorithmicThresholding\ #1}
\algrenewtext{EndThresholding}{\algorithmicendThresholding}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\algblock{Update}{EndUpdate}
\algnewcommand\algorithmicUpdate{\textbf{\em Sampling Distribution Update}}
 \algnewcommand\algorithmicendUpdate{}
\algrenewtext{Update}[1]{\algorithmicUpdate\ #1}
\algrenewtext{EndUpdate}{\algorithmicendUpdate}
%%%%%%%%%%%%%%%%%%%
\algtext*{EndCandidate}
\algtext*{EndEstimation}
\algtext*{EndElite}
\algtext*{EndThresholding}
%%%%%%%%%%%%%%%%%%%

\begin{algorithm}
\begin{algorithmic}
\State {\bf Input:}  family of distributions $\{f(\cdot,\eta)\}$, initial parameter vector $\eta_0$ s.t. $f(\theta,\eta_0)>0 ~\forall\, \theta\in \theta$, trajectory lengths $\{m_n\}$, 
$\rho_0 \in (0,1]$, $N_0>1$,
$\varepsilon> 0$, $\alpha>1$, $\lambda \in(0,1)$,
strictly decreasing function
$\mathcal{H}\hspace*{-3pt}$.

\For{$n = 0,1,2,\ldots$}	
	\Candidate
	    \State 
	    Generate $N_n$ policies using the mixed distribution $\widetilde f(\cdot,\eta_n)= (1-\lambda)f(\cdot,\widetilde\eta_n)+\lambda f(\cdot,\eta_0)$. 
	    \State Denote these candidate policies by $\Lambda_n=\{\theta^1_n, \ldots, \theta^{N_n}\}$.
	\EndCandidate    
	\Estimation
	    \For{$i = 1,2,\ldots,N_n$}
	      \State Simulate $m_n$ samples using policy $\theta^i_n$
	      \State Obtain CPT-value estimate $\overline \C_n^{\theta^i_n}$ using Algorithm \ref{alg:holder-est}
	      \EndFor
	\EndEstimation
	%%%%%%%%%%%%%%%%%%%%%%%%%%55
	\Elite
	  \State Order the CPT-value estimates\footnotemark[1] $\{\overline \C_n^{\theta^{(1)}_n},\ldots,\overline \C_n^{\theta^{(N_n)}_n}\}$. 
	  \State Compute the $(1-\rho_n)$-quantile from the above samples as follows: 
	  \begin{align}
\widetilde \chi_{n}(\rho_n,N_n) = \overline \C_n^{\theta^{\lceil(1-\rho_k)N_k \rceil}_n}.\label{eq:quant}
\end{align}
	\EndElite
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\Thresholding
	\If {$n=0$ or $\widetilde\chi_{n}(\rho_n,N_n)\geq \bar\chi_{n-1}+\varepsilon$}
	    \State Set $\bar \chi_{k} = \widetilde \chi_{k}(\rho_n,N_n),~\rho_{k+1} = \rho_n,~N_{k+1} = N_{k}$ and \label{step:3a}
	    \State Set $\theta^*_{n} = \theta_{1-\rho_{n}}$, where $\theta_{1-\rho_{n}}$ is the policy that corresponds to the $(1-\rho_n)$-quantile in \eqref{eq:quant}.
	\Else
             \State find the largest $\bar \rho \in (0, \rho_n)$ such that $\widetilde\chi_{n}(\bar \rho,N_n)\geq \bar\chi_{n-1}+\varepsilon$;             
             \If {$\bar \rho$ exists} 
              \State Set $\bar \chi_{n} = \widetilde \chi_{n}(\bar \rho,N_n),~ \rho_{k+1}  = \bar \rho,~N_{n+1} = N_{n}$ and
              $\theta^*_{n} = \theta_{1- \bar \rho}$ \label{step:3b}
              \Else
	      \State Set $\bar \chi_{n}  = \widehat V_n^{\theta^*_{n-1}}(x^0)$,$\rho_{n+1} = \rho_n$,$N_{n+1} = \lceil\alpha N_{n}\rceil,$ and
          $\theta^*_{n} = \theta^*_{n-1}$.\label{step:3c}
	      \EndIf
         \EndIf
	\EndThresholding
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
	    \Update
		\State Parameter update\footnotemark[2]:  
		                         \begin{equation*} 
\eta_{n+1} \in \argmin_{\eta \in \C}\frac{1}{N_n}\sum_{i=1}^{N_n}\frac{[\mathcal{H}(\overline \C_n^{\theta^i_n})]^n}{\widetilde f(\theta,\eta_n)}
\widetilde I\big(\overline \C_n^{\theta^i_n},\bar \chi_{n}\big) \ln f(\theta,\eta).
                            \end{equation*}

		\EndUpdate
\EndFor
\State {\bf Return} $\theta_n$
\end{algorithmic}
\caption{Structure of  GF-CPT-MPS algorithm.}
\label{alg:mras}
\end{algorithm}

\footnotetext[1]{Here $\widehat V_n^{\theta^{(i)}_n}(x^0)$ denotes the $i$th order statistic.}
\footnotetext[2]{Here $\widetilde I(z,\chi):=\left\{\begin{array}{ll}
                                                          0 & ~\mbox{if $z\leq \chi-\varepsilon$,} \\
									    (z-\chi+\varepsilon)/ \varepsilon & ~\mbox{if $\chi-\varepsilon<z<\chi$,}\\
                                                          1 & ~\mbox{if $z\geq \chi$.}
                                                          \end{array} \right.$}
                    
Finally, we get to our setting where we only obtain a biased estimate of the CPT-value $\C(X^\theta)$ for any policy $\theta$. Recall that the bias is due to a finite sample run followed by the estimation scheme outlined in Algorithm \ref{alg:holder-est}. As in the case of SPSA-based algorithms, it is easy to see that the number of samples $m_n$ (in iteration $n$) should asymptotically increase to infinity. Assuming this setup, the gradient-free model-based policy search algorithm would involve the following steps (see Algorithm \ref{alg:mras} for the pseudocode):

\begin{description}
 \item[Step 1 (Candidate policies):] Generate $N_n$ policies $\{\theta^1_n, \ldots, \theta^{N_n}_n\}$ using the distribution $f(\cdot,\eta_n)$.

\item[Step 2 (CPT-value estimation):] Obtain $m_n$ samples for each of the parameters in $\theta^i_n, i=1,\ldots, N_n$ and return CPT-value estimates $\overline \C_n^{\theta^i_n}$.

\item[Step 3 (Parameter update):]
                         \begin{equation} \label{eqn:step4}
                           \eta_{n+1} \in \argmin_{\eta \in \C}\frac{1}{N_n}\sum_{i=1}^{N_n}\frac{[\mathcal{H}( \overline \C_n^{\theta^i_n})]^n}{ f(\theta_n^i,\eta_n)}\ln f(\theta_n^i,\eta).
                            \end{equation}

\end{description}

                    
A few remarks are in order.
\begin{remark}(\textbf{\textit{Choice of sampling distribution}})
A natural question is how to compute the KL-distance \eqref{eqn:kl} in order to update the policy. A related question is how to choose the family of distributions $f(\cdot,\theta)$, so that the update \eqref{eqn:interior} can be done efficiently. One choice is to employ the natural exponential family (NEF) since it ensures that the KL distance in \eqref{eqn:kl} can be computed analytically. %See Appendix \ref{appendix:mras} for details.
\end{remark}

\begin{remark}(\textbf{\textit{Elite sampling}})
In practice, it is efficient to use only an elite portion of the candidate policies that have been sampled in order to update the sampling distribution $f(\cdot,\eta)$. This can be achieved by using a quantile estimate of the CPT-value function corresponding to candidate policies that were estimated in a particular iteration. The intuition here is that using policies that have performed well guides the policy search procedure towards better regions more efficiently in comparison to an alternative that uses all the candidate policies for updating $\eta$.
\end{remark}

\subsection{Convergence result}
\begin{theorem}\label{thm:mras}
%Let $\varphi>0$ be a positive constant satisfying the condition that the set $\big\{\theta:\mathcal{H}(\C(D^\theta)\geq \frac{1}{\varphi} \big\}$ has a strictly positive Lebesgue/counting measure\index{Lebesgue measure}\index{counting measure}.
Assume (A1)-(A2). Suppose that multivariate normal densities are used for the sampling distribution, i.e., $\eta_n = (\mu_n, \Sigma_n)$, where $\mu_n$ and $\Sigma_n$ denote the mean and covariance of the normal densities.
Then, 
\begin{equation}\label{eqn:smain}
\lim_{n\rightarrow \infty}\mu_n=\theta^* \text{ and } \lim_{n\rightarrow \infty}\Sigma_n=0_{d\times d}~~a.s.
\end{equation}
\end{theorem}
\begin{proof}
 See Section \ref{appendix:mras}.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence Proofs}
\label{sec:convergence}
\input{proofs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulation Experiments}
\label{sec:expts}

\subsection{Simulation Setup}  
We consider a SSP version of an example\footnote{A similar example has been considered in \cite{chow2014algorithms}.} for buying a house at the optimal price. Suppose the house is priced at $x_k$ at any instant $k$ and at the next instant, the price either goes down to $\left(x_k \times C_{down}\right)$ w.p. $p_{down}$ or goes up to $\left(x_k\times C_{up}\right)$ w.p. $1-p_{down}$. The actions are to either wait (denoted $w$), which results in a holding cost $h$ or to buy (denoted $b$) at the current price. The horizon is capped at $T$, with a terminal cost $x_T$.  The goal is to minimize the total cost defined as $ 
D^{\theta}(x^0)= \sum_{k=0}^\tau \left(I_{\{a_k =b \} }x_k+I_{\{a_k =w \} } h\right) + I_{\{\tau=T\}} x_T$, where $\tau =  \{k | \theta(x_k)=1 \} \wedge T$.
We set $T=20, h=0.1, C_{up}=2, C_{down}=0.5$, and $x_0=1$.  

 \begin{figure*}
    \centering
     \begin{tabular}{cc}
\begin{subfigure}[b]{0.5\textwidth}
\tabl{c}{\scalebox{0.7}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend pos=north east,
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle
(-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw
(-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={\bf Expected value},
xlabel={\bf Probability $\bm{p_{down}}$},
xtick=data,
ytick align=outside,
xticklabel style={align=center},
ymin=0.1,
ymax=1.4,
bar width=14pt,
nodes near coords,
grid,
grid style={gray!30},
width=11cm,
height=9.5cm,
]
\addplot[fill=red!20]  table[x index=0, y index=1, col sep=comma] {results/Valueiteration.txt} ;
\addlegendentry{Value iteration}
\end{axis}
\end{tikzpicture}}\\[1ex]}
\caption{Value iteration}
\label{fig:vi}
\end{subfigure}
&
\begin{subfigure}[b]{0.5\textwidth}
\tabl{c}{\scalebox{0.7}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend pos=north east,
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle
(-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw
(-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={\bf CPT-Value},
xlabel={\bf Probability $\bm{p_{down}}$},
xtick=data,
ytick align=outside,
xticklabel style={align=center},
bar width=14pt,
ymin=0.1,
ymax=1.4,
nodes near coords,
grid,
grid style={gray!30},
width=11cm,
height=9.5cm,
]
\addplot[fill=blue!20]  table[x index=0, y index=1, col sep=comma] {results/twospsa_cpt.txt} ;
\addlegendentry{CPT-SPSA-N}
\end{axis}
\end{tikzpicture}}\\[1ex]}
\caption{Second-order SPSA for CPT-value}
\label{fig:cpt2spsa}
\end{subfigure}
\\
\begin{subfigure}[b]{0.5\textwidth}
    \tabl{c}{\scalebox{0.7}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend pos=north east,
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle
(-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw
(-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={\bf Expected value},
xlabel={\bf Probability $\bm{p_{down}}$},
xtick=data,
ytick align=outside,
xticklabel style={align=center},
ymin=0.1,
ymax=1.8,
bar width=14pt,
nodes near coords,
grid,
grid style={gray!30},
width=11cm,
height=9.5cm,
]
\addplot[fill=yellow!30]  table[x index=0, y index=1, col sep=comma] {results/SPSADETERMINISTIC.txt} ;
\addlegendentry{NoCPT-SPSA-G}
\end{axis}
\end{tikzpicture}}\\[1ex]}
\caption{SPSA for regular value function}
\label{fig:nocptspsa}
\end{subfigure}
&
\begin{subfigure}[b]{0.5\textwidth}
\tabl{c}{\scalebox{0.7}{\begin{tikzpicture}
\begin{axis}[
ybar={2pt},
legend pos=north east,
legend image code/.code={\path[fill=white,white] (-2mm,-2mm) rectangle
(-3mm,2mm); \path[fill=white,white] (-2mm,-2mm) rectangle (2mm,-3mm); \draw
(-2mm,-2mm) rectangle (2mm,2mm);},
ylabel={\bf CPT-Value},
xlabel={\bf Probability $\bm{p_{down}}$},
xtick=data,
ytick align=outside,
xticklabel style={align=center},
bar width=14pt,
ymin=0.1,
ymax=1.4,
nodes near coords,
grid,
grid style={gray!30},
width=11cm,
height=9.5cm,
]
\addplot[fill=green!20]  table[x index=0, y index=1, col sep=comma] {results/SPSACPT.txt} ;
\addlegendentry{CPT-SPSA-G}
\end{axis}
\end{tikzpicture}}\\[1ex]}
\caption{First-order SPSA for CPT-value}
\label{fig:cptspsa}
\end{subfigure}
\end{tabular}
\caption{Performance of policy gradient algorithms with/without CPT for different down probabilities of the SSP}
\label{fig:perf}
\end{figure*}


  

\paragraph{Implementation:} On this example, we implement the first-order CPT-SPSA-G and the second-order CPT-SPSA-N algorithms. For the sake of comparison, we also apply value iteration to the SSP example described above. 
Note that value iteration requires knowledge of the model, while our CPT based algorithms estimate CPT-value using simulated episodes.
We implement the algorithm from \cite{bhatnagar2004simultaneous} for the SSP example described in the numerical experiments of the main paper. The latter, henceforth referred to as NoCPT-SPSA-G, is an SPSA-based scheme that optimizes the traditional value function objective in a discounted MDP setting and we make a trivial adaptation of this algorithm for the SSP setting.
For CPT-SPSA-G and NoCPT-SPSA-G, we set $\delta_n = 1.9/n^{0.101}$ and $\gamma_n = 1/n$, while for CPT-SPSA-N, we set $\delta_n=3.8/n^{0.166}$ and $\gamma_n=1/n^{0.6}$. For all algorithms, we set each entry of the initial policy $\theta_0$ to $0.1$. For CPT-value estimation, we simulate $1000$ SSP episodes, with the SSP horizon $T$ set to $20$. All algorithms are run with a budget of $1000$ samples, which implies $500$ iterations of CPT-SPSA-G and $333$ iterations of CPT-SPSA-N. The results presented are averages over $500$ independent simulations. For CPT-SPSA-G/CPT-SPSA-N, 
the weight functions $w^+$ and $w^-$ are set to $p^{0.6}/(p^{0.6}+(1-p)^{0.6})$, while the utility functions are identity maps. 



\subsection{Results} Figures \ref{fig:vi}--\ref{fig:nocptspsa} present the value function computed using value iteration and NoCPT-SPSA-G, while Figures \ref{fig:cptspsa}--\ref{fig:cpt2spsa} present the CPT-value $V^{\theta_{end}}(x^0)$ for CPT-SPSA-G and CPT-SPSA-N, respectively. The performance plots are for various values of $p_{down}$, the probability of house price going down. 
From Figure \ref{fig:vi}, we notice that the variations in expected total cost is larger in comparison to that in CPT-value. Figure \ref{fig:nocptspsa} implies that a similar observation about variation of expected value holds true for NoCPT-SPSA-G algorithm from \cite{bhatnagar2004simultaneous}. While it is difficult to plot the entire policies, for the expected value minimizing algorithms it was observed that there were drastic changes in the policies with a change of $0.01$ in $p_{down}$, while PG/CPT-SPSA-N resulted in randomized policies that smoothly transitioned with changes in $p_{down}$.
As motivated in the introduction, these plots verify that CPT-aware SPSA algorithms are less sensitive to the model changes as compared to the expected value minimizing algorithms. It is also evident that the second-order CPT-SPSA-N gives marginally better results than its first-order counterpart CPT-SPSA-G.
 Finally, what is not shown is that the CPT-value obtained for PG/CPT-SPSA-N is much lower than that obtained for NoCPT-SPSA-G, thus making apparent the need for specialized algorithms that incorporate CPT-based criteria.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Numerical Experiments for CPT-value estimation scheme}
% \label{sec:simulation}
% 
% \paragraph{Setup.} We consider a random variable $X$ that is uniformly distributed in $[0,5]$.
% We set the utility function to be identity 
% and define the weight function $w$ as follows:
% \[w(x)= 
%      \begin{cases}
%      \frac{2}{3} (2x-x^2)  & 0 \leq x < \frac{1}{2} \\
%      \frac{1}{3}+ \frac{2}{3} x^2& \frac{1}{2} \leq x \leq 1
%      \end{cases}
% \]
% The graph of $w(x)$ can be seen in Fig. \ref{fig:w1}. 
% Thus, the CPT-value of $X$ can be seen to be
% \begin{align}
% \C(X) = \int_0^{\infty} w(P(X>x)) dx,
% \label{eq:vx}
% \end{align}
% Since we consider gains only, the second integral component in $\C(X)$ from \eqref{eq:cpt-val} is zero.
% 
%  \begin{figure}
%     \centering
% \tabl{c}{
% %\includegraphics[width=8cm]{results/Probability_weighting_function.jpg}
%   \scalebox{1.0}{\begin{tikzpicture}
%   \begin{axis}[width=8cm,height=7.35cm,legend pos=south east,
%            grid = major,
%            grid style={dashed, gray!30},
%            xmin=0,     % start the diagram at this x-coordinate
%            xmax=1,    % end   the diagram at this x-coordinate
%            ymin=0,     % start the diagram at this y-coordinate
%            ymax=1,   % end   the diagram at this y-coordinate
%            axis background/.style={fill=white},
%            ylabel={\large Weight $w(p)$},
%            xlabel={\large Probability $p$}
%            ]
%           \addplot[domain=0:0.5, red, thick] 
%              {2/3*(2*x - x*x)}; 
%            \addplot[domain=0.5:1, red, thick, smooth]           {1/3 + 2/3 * x*x}; 
%            \addlegendentry{$w(p)$}
%                  \addplot[domain=0:1, blue, thick]           {x}; 
%   \end{axis}
%   \end{tikzpicture}}\\[1ex]
% }
% \caption{Weight function}
% \label{fig:w1}% \end{minipage}
% \end{figure}
% 
% 
% \paragraph{Background.}
% Let $X_1,\ldots, X_n$ be i.i.d. random variables with underlying distribution $U[0,5]$.  
% Then, the empirical distribution function is defined as
% \begin{align}
% {\hat F_n}(x)= \frac{1}{n} \sum_{i=1}^n 1_{(X_i \leq x)}.
% \label{eq:edf}
% \end{align}
% Thus,  $1-{\hat F_n}(x)$ is an unbiased estimator of $P(X>x)$. 
% From \eqref{eq:edf}, it is clear that ${\hat F_n}(x)$ generates a Lebesgue Stieljes measure which takes mass $\frac{1}{n}$ at each of the points $X_i, i=1,\ldots,n$. So does $w(1-{\hat F_n}(x))$. Based on this observation, one can see the weight function equivalently as follows:
% $$ w(1-{\hat F_n}(x)) =\int_x^{\infty} d w(1-{\hat F_n}(y))$$
% 
% % Observe also that the integral 
% % $$
% % \int_0^{+\infty} \wfn dx(\omega)
% % $$
% % always exists because $w^+$ is bounded and $\wfn$ takes values in a finite support.
% 
% Hence, the estimate $\widehat{V_n}(X)$ of $\C(X)$ is arrived at as follows:
% \begin{align}
% \widehat V_n(X)= \int_0^{\infty} w(1-{\hat F_n}(x)) dx=& - \intinfinity  \int_x^\infty d w(1-{\hat F_n}(y)) \\
% =& - \intinfinity  x d w(1-\hat{F_n}(x))\nonumber\\
% =&\sum_{i=1}^n X_{[i]} \left(w\left(\frac{i+1}{n}\right)- w\left(\frac{i}{n}\right)\right),\label{eq:c1}
% \end{align}
% where $X_{[i]}$ denotes the $i$th order statistic of the sample set $\{X_1,\ldots,X_n\}$. Note that, we let $w^+\left(\frac{n+1}{n}\right)=1$, $\forall n$.
% 
% We use \eqref{eq:c1} to estimate the CPT-value $\C(X)$. Analytically, $\C(X)=2.5$ for a $U[0,5]$ distributed random variable $X$.
% In the following, we report the accuracy of the estimator $\widehat{V}_n(X)$ using simulation experiments.
% 
%  \begin{figure}
%     \centering
% \tabl{c}{\scalebox{1.0}{\begin{tikzpicture}
% \begin{axis}[xlabel={number of samples $n$},ylabel={$\left| \overline \C_n - \C(X) \right|$}, width=8cm,height=7.35cm,ytick pos=left,xtick pos=left,grid,grid style={gray!30}]
% \addplot table[x index=0,y index=1,col sep=comma] {results/finaldata.txt};
% %\addlegendentry{$\l\theta_{k} - \hat\theta_T\r^2$}% y index+1 since humans count from 1
% \end{axis}
% \end{tikzpicture}}\\[1ex]}
% \caption{Weight function and estimation error for a $U[0,5]$ random variable.}
% \label{fig:perf}
% \end{figure}
% %In order to visualize the convergence rate of the empirical estimation scheme in \eqref{eq:cpt-est}, we set up $1000$ macro-simulations, with the number of samples used ranging from $100$ to $100,000$. 
% \paragraph{Results.} Fig. \ref{fig:perf} shows the estimation error obtained for a random variable $X$ with distribution $U[0.5]$. Here, the estimation error denotes the absolute difference between the estimated CPT-value \eqref{eq:c1} and true CPT-value, which is $2.5$. 
% From Fig. \ref{fig:perf}, it is evident that the CPT-value estimation scheme \eqref{eq:c1} converges rapidly to the true CPT-value.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions and Future Work}
\label{sec:conclusions}
CPT has been a very popular paradigm for modeling human decisions among psychologists/economists, but has escaped the radar of the AI community. This work is the first step in incorporating CPT-based criteria into an RL framework. However, both estimation and control of CPT-based value is challenging. Using temporal-difference learning type algorithms for estimation was ruled out for CPT-value since the underlying probabilities get (non-linearly) distorted by a weight function. Using empirical distributions, we proposed an estimation scheme that converges at the optimal rate. Next, for the problem of control, since CPT-value does not conform to any Bellman equation, we employed SPSA - a popular simulation optimization scheme and designed both first and second-order algorithms for optimizing the CPT-value function. 
We provided theoretical convergence guarantees for all the proposed algorithms. We illustrated the usefulness of CPT-based criteria in a numerical example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}

% \bibliographystyle{jsr}
\bibliography{cpt-refs}

\end{document}


